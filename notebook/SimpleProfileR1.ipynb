{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f945bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from nni.algorithms.compression.pytorch.quantization import LsqQuantizer, QAT_Quantizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.numpy_helper\n",
    "from math import ceil\n",
    "### markus\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "from re import L\n",
    "import subprocess\n",
    "from subprocess import DEVNULL, STDOUT\n",
    "from xmlrpc.client import boolean\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "import myprofC as prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSizeW= 128#1280\n",
    "\n",
    "imgSizeH= 128#720\n",
    "gen_img_input_dim_w = imgSizeW\n",
    "gen_img_input_dim_h = imgSizeH\n",
    "gen_img_input_channels = 8\n",
    "#test input image\n",
    "test_input_data=\"../convertdemo/dataset/rand_3.jpg\"\n",
    "#Paths\n",
    "quant_image_path = \"../quantization_images\"\n",
    "script_path = \"../scripts\"\n",
    "log_path = \"../logs\"\n",
    "network_path = \"../convertdemo/network\"\n",
    "#Files\n",
    "perform_script = \"perform_r6.sh\"\n",
    "parse_script = \"parse_r1.sh\"\n",
    "perform_log_file = \"model_execution.log\"\n",
    "parsed_log_file = \"model_execution_parsed.log\"\n",
    "model_name=\"mnist\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3c22",
   "metadata": {},
   "source": [
    "# Generate images based on some arbitrary input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4177221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = quant_image_path\n",
    "\n",
    "def generate_random_images(xdim, ydim, channels=3, count=1, path=\".\"):\n",
    "    \"\"\"\n",
    "    This functions generates random bmp images to use for quantization given\n",
    "    a defined dimension\n",
    "        @xdim   .. width of images\n",
    "        @ydim   .. height of images\n",
    "        @count  .. number of images\n",
    "        @path   .. path of images\n",
    "    \"\"\"\n",
    "\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(path)\n",
    "        print(\"The new directory quantization_images is created!\")\n",
    "\n",
    "    #delete the pre generated bmp/jpg files\n",
    "    filelist = [ f for f in os.listdir(quant_image_path) if (f.endswith(\".jpg\") or f.endswith(\".bmp\") ) ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(quant_image_path, f))\n",
    "        \n",
    "    for c in range(count):\n",
    "        rnd_img = np.random.randint(low=0,high=255, size=(ydim, xdim, channels),dtype=np.uint8) #imag.transpose((1,2,0)\n",
    "        imag_tp = np.ascontiguousarray(rnd_img, dtype=np.uint8)\n",
    "\n",
    "        pil_image = PIL.Image.frombytes('RGB',(xdim, ydim), imag_tp)\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".bmp\")\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b28be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "# choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## generate 10 random input images based on the provided dimensions\n",
    "generate_random_images(gen_img_input_dim_w, gen_img_input_dim_h, gen_img_input_channels, count=20, path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71df62",
   "metadata": {},
   "source": [
    "# Build the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a3bd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(Mnist, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(num_channels, 20, 5, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels = input_channels, out_channels = out_channels, kernel_size = 5, stride = 1)\n",
    "        # self.conv2 = nn.Conv2d(20, 20, 5, 1)\n",
    "\n",
    "        # self.conv3 = nn.Conv2d(20, 50, 1, 1)\n",
    "\n",
    "        # f1 = int(imgSizeX/4) - 3\n",
    "        # f2 = int(imgSizeY/4) - 3\n",
    "        # self.fc1 = nn.Linear(f1*f2*50, 500)\n",
    "        # self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = F.max_pool2d(x, 2, 2)\n",
    "        # x = F.relu(self.conv2(x))\n",
    "\n",
    "        # x = F.relu(self.conv3(x))\n",
    "\n",
    "        # x = F.max_pool2d(x, 2, 2)\n",
    "        # f1 = int(imgSizeX/4) - 3\n",
    "        # f2 = int(imgSizeY/4) - 3\n",
    "        # x = x.view(-1, f1*f2*50)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d7aa64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(gen_img_input_dim_w*gen_img_input_dim_h, 50)#gen_img_input_dim_w*gen_img_input_dim_h\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # select first dim from [1,C,H,W] ie. [0,:,:,:]\n",
    "        x = x.select(0,0)\n",
    "        #print(x.shape)\n",
    "        # select first channel [0,:,:]\n",
    "        x = x.select(0,0)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape((1,-1))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "368a16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SimpleNN()\n",
    "model = Mnist(gen_img_input_channels, 64)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d67f1",
   "metadata": {},
   "source": [
    "# Train the model for random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa6de45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_images(count=1, path=\".\", extension='*.bmp'):\n",
    "    \"\"\"\n",
    "    loads images as np arrays; no normalization\n",
    "    \"\"\"\n",
    "    imgs =  []\n",
    "    files = glob.glob(path + \"/\" + extension)\n",
    "    for i, f in enumerate(files):\n",
    "        img = np.asarray(PIL.Image.open(f), dtype=np.float32).transpose((2,0,1))\n",
    "        img = img.reshape( (1,img.shape[0],img.shape[1], img.shape[2]) )\n",
    "        img = torch.from_numpy(img)\n",
    "        imgs.append(img)\n",
    "\n",
    "        if i+1 > count:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = load_images(count=2,path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a40f3c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for img in imgs:\n",
    "#     target = torch.tensor([2])\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(img)\n",
    "#     #print(output)\n",
    "#     loss = F.nll_loss(output, target)\n",
    "#     print(loss.item())\n",
    "#     #print(target, output)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "133ee91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Exporting to onnx\n",
      "Exported graph: graph(%input : Float(1, 8, 128, 128, strides=[131072, 16384, 128, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(64, 8, 5, 5, strides=[200, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(64, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %onnx::Relu_3 : Float(1, 64, 124, 124, strides=[984064, 15376, 124, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_0\"](%input, %conv1.weight, %conv1.bias) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %output : Float(1, 64, 124, 124, strides=[984064, 15376, 124, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_1\"](%onnx::Relu_3) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  return (%output)\n",
      "\n",
      "------------- Checking exported model\n",
      "------------- Performing the profiling...\n",
      "\u001b[35m>>>>> Setting Environment Variables Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Entering convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "2022-10-08 13:44:43.635936: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-08 13:44:43.635968: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Start importing onnx...\u001b[0m\n",
      "\u001b[32mI Current ONNX Model use ir_version 3 opset_version 7\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'eliminate_option_const' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_const_branch' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_if' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_sequence_construct_concat_from_sequence' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_lrn_lowlevel_implement' success\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_conv1.weight shape: [64, 8, 5, 5]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_conv1.bias shape: [64]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Relu_3 shape: [1, 64, 124, 124]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_output shape: [1, 64, 124, 124]\u001b[0m\n",
      "\u001b[32mI build output layer attach_Relu_Relu_1:out0\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_1:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_1']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_0:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_conv1.bias', 'Initializer_conv1.weight', 'Conv_Conv_0']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI build input layer input:out0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_0_2 0  ~ Relu_Relu_1_1 0\u001b[0m\n",
      "\u001b[36mD connect input_3 0  ~ Conv_Conv_0_2 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_1_1 0  ~ attach_Relu_Relu_1/out0_0 0\u001b[0m\n",
      "2022-10-08 13:44:45.113723: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-08 13:44:45.137983: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3792905000 Hz\n",
      "2022-10-08 13:44:45.138501: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x616ca70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-08 13:44:45.138532: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-08 13:44:45.140398: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-08 13:44:45.140422: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-08 13:44:45.140450: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (881b0c9b4528): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36mD Process input_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 8 128 128)\u001b[0m\n",
      "\u001b[36mD Tensor @input_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_0_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_0_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_1_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_1_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Relu_Relu_1/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Relu_Relu_1/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Start C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Optimizing network with broadcast_op\u001b[0m\n",
      "\u001b[36mD insert permute attach_Relu_Relu_1/out0_0_acuity_mark_perm_4 before attach_Relu_Relu_1/out0_0\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_0_2_acuity_mark_perm_5 before Conv_Conv_0_2\u001b[0m\n",
      "\u001b[32mI End C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Process input_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 8 128 128)\u001b[0m\n",
      "\u001b[36mD Tensor @input_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_0_2_acuity_mark_perm_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 128 8)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_0_2_acuity_mark_perm_5:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_0_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 124 124 64)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_0_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_1_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 124 124 64)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_1_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Relu_Relu_1/out0_0_acuity_mark_perm_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Relu_Relu_1/out0_0_acuity_mark_perm_4:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Relu_Relu_1/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Relu_Relu_1/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[36mD Optimizing network with force_1d_tensor, swapper, merge_duplicate_quantize_dequantize, merge_layer, auto_fill_bn, auto_fill_l2normalizescale, auto_fill_instancenormalize, resize_nearest_transformer, auto_fill_multiply, compute_gather_negative, auto_fill_zero_bias, proposal_opt_import, special_add_to_conv2d, extend_gather_to_gather_reshape\u001b[0m\n",
      "\u001b[32mI End importing onnx...\u001b[0m\n",
      "\u001b[32mI Dump net to mnist.json\u001b[0m\n",
      "\u001b[32mI Save net to mnist.data\u001b[0m\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "-------------------- QUANTIZATION SCRIPT\n",
      "2022-10-08 13:44:45.749659: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-08 13:44:45.749691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Namespace(action='quantization', batch_size=1, caffe_mean_file=None, capture_format='nchw', capture_quantized=False, channel_mean_value='127 127 127 255', config=None, data_output=None, debug=True, device=None, divergence_first_quantize_bits=11, dtype='float', epochs=1, epochs_per_decay=100, force_gray=False, fpfs_delta0=1, fpfs_epochs=0, fpfs_reduce_target=0, input_fitting='scale', input_normalization=None, lr=0.1, mean_file=None, model_data='mnist.data', model_data_format='zone', model_input='mnist.json', model_quantize=None, optimizer='momentum', output_dir=None, output_num=5, pb_name=None, pfps_delta0=1, pfps_epochs=0, pfps_reduce_target=0, prune_epochs=10, prune_loss=1, quantized_algorithm='normal', quantized_divergence_nbins=0, quantized_dtype='asymmetric_affine-u8', quantized_hybrid=False, quantized_moving_alpha=0.0, quantized_rebuild=False, quantized_rebuild_all=True, random_brightness=None, random_contrast=None, random_crop=False, random_flip=False, random_mirror=False, reorder_channel=None, restart=False, samples=-1, source='text', source_file='dataset/dataset1.txt', task='classification', validation_output='validation.csv', without_update_masked_grad=False)\u001b[0m\n",
      "2022-10-08 13:44:47.082095: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-08 13:44:47.105985: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3792905000 Hz\n",
      "2022-10-08 13:44:47.106503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58c1c90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-08 13:44:47.106529: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-08 13:44:47.108389: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-08 13:44:47.108416: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-08 13:44:47.108435: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (881b0c9b4528): /proc/driver/nvidia/version does not exist\n",
      "\u001b[32mI Load model in mnist.json\u001b[0m\n",
      "\u001b[32mI Load data in mnist.data\u001b[0m\n",
      "W:tensorflow:From acuitylib/app/tensorzone/workspace.py:26: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "\u001b[32mI Fitting image with scale.\u001b[0m\n",
      "\u001b[32mI Channel mean value [127.0, 127.0, 127.0, 255.0]\u001b[0m\n",
      "\u001b[32mI [TRAINER]Quantization start...\u001b[0m\n",
      "[TRAINER]Quantization start...\n",
      "\u001b[32mI Init validate tensor provider.\u001b[0m\n",
      "\u001b[32mI Enqueue samples 20\u001b[0m\n",
      "\u001b[32mI Init provider with 20 samples.\u001b[0m\n",
      "\u001b[36mD set up a quantize net\u001b[0m\n",
      "\u001b[36mD Process input_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 8 128 128)\u001b[0m\n",
      "\u001b[36mD Tensor @input_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 8, 128, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_0_2_acuity_mark_perm_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 128 8)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_0_2_acuity_mark_perm_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 128, 8)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_0_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 124 124 64)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_0_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 124, 124, 64)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_1_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 124 124 64)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_1_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 124, 124, 64)\u001b[0m\n",
      "\u001b[36mD Process attach_Relu_Relu_1/out0_0_acuity_mark_perm_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Relu_Relu_1/out0_0_acuity_mark_perm_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 64, 124, 124)\u001b[0m\n",
      "\u001b[36mD Process attach_Relu_Relu_1/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 64 124 124)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Relu_Relu_1/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 64, 124, 124)\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Generated network graph with 1 outputs.\u001b[0m\n",
      "\u001b[32mI  @attach_Relu_Relu_1/out0_0:out0: (1, 64, 124, 124)\u001b[0m\n",
      "\u001b[36mD Init coefficients ...\u001b[0m\n",
      "\u001b[32mI Start tensor porvider ... \u001b[0m\n",
      "\u001b[32mI Runing 1 epochs, algorithm: normal\u001b[0m\n",
      "\u001b[32mI iterations: 0\u001b[0m\n",
      "2022-10-08 13:44:47.468560: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at queue_op.cc:109 : Invalid argument: Shape mismatch in tuple component 0. Expected [8,128,128], got [3,128,128]\n",
      "Traceback (most recent call last):\n",
      "  File \"tensorflow/python/client/session.py\", line 1365, in _do_call\n",
      "  File \"tensorflow/python/client/session.py\", line 1350, in _run_fn\n",
      "  File \"tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
      "tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)\n",
      "\t [[{{node fifo_queue_DequeueMany}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"tensorzonex.py\", line 445, in <module>\n",
      "  File \"tensorzonex.py\", line 386, in main\n",
      "  File \"acuitylib/app/tensorzone/quantization.py\", line 176, in run\n",
      "  File \"acuitylib/app/tensorzone/quantization.py\", line 154, in _run_quantization\n",
      "  File \"acuitylib/app/tensorzone/graph.py\", line 98, in run\n",
      "  File \"tensorflow/python/client/session.py\", line 958, in run\n",
      "  File \"tensorflow/python/client/session.py\", line 1181, in _run\n",
      "  File \"tensorflow/python/client/session.py\", line 1359, in _do_run\n",
      "  File \"tensorflow/python/client/session.py\", line 1384, in _do_call\n",
      "tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_fifo_queue' is closed and has insufficient elements (requested 1, current size 0)\n",
      "\t [[node fifo_queue_DequeueMany (defined at tensorflow/python/framework/ops.py:1949) ]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_DequeueMany':\n",
      "  File \"tensorzonex.py\", line 445, in <module>\n",
      "  File \"tensorzonex.py\", line 386, in main\n",
      "  File \"acuitylib/app/tensorzone/quantization.py\", line 176, in run\n",
      "  File \"acuitylib/app/tensorzone/quantization.py\", line 116, in _run_quantization\n",
      "  File \"acuitylib/app/tensorzone/workspace.py\", line 180, in _setup_graph\n",
      "  File \"acuitylib/app/tensorzone/tensorprovider.py\", line 159, in get_output\n",
      "  File \"tensorflow/python/ops/data_flow_ops.py\", line 488, in dequeue_many\n",
      "  File \"tensorflow/python/ops/gen_data_flow_ops.py\", line 3569, in queue_dequeue_many_v2\n",
      "  File \"tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\n",
      "  File \"tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\n",
      "  File \"tensorflow/python/framework/ops.py\", line 1949, in __init__\n",
      "\n",
      "[67] Failed to execute script tensorzonex\n",
      "\u001b[35m>>>>> Exiting convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "patching file main.c\n",
      "\u001b[35m>>>>> Patching main.c on Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_pre_process.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_mnist.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vnn_mnist.c: In function ‘vnn_CreateMnist’:\n",
      "vnn_mnist.c:146:29: warning: unused variable ‘data’ [-Wunused-variable]\n",
      "  146 |     uint8_t *               data;\n",
      "      |                             ^~~~\n",
      "At top level:\n",
      "vnn_mnist.c:94:17: warning: ‘load_data’ defined but not used [-Wunused-function]\n",
      "   94 | static uint8_t* load_data\n",
      "      |                 ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COMPILE /home/khadas/nbg_unify_mnist/main.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_post_process.c\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing network on Khadas is saved in model_execution.log <<<<<<<\u001b[37m\n",
      "------------- Performing the profiling done!\n",
      "Return state run_profiler: \n",
      "0\n",
      "------------- Parsing the profiling results...\n",
      "Parsing ../logs/model_execution.log....\n",
      "------------- Parsing the profiling results done!\n",
      "Return state parse_the_results: \n",
      "0\n",
      "------------- auto_profile done!...\n"
     ]
    }
   ],
   "source": [
    "loop_run = '10'\n",
    "\n",
    "[ProfileArray,status] = prof.doProfiling(model,\n",
    "                 loop_run,\n",
    "                 gen_img_input_channels,\n",
    "                 gen_img_input_dim_h,\n",
    "                 gen_img_input_dim_w,\n",
    "                 debug=True)\n",
    "\n",
    "accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ec76fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_call(['../scripts/sajjad.sh'])\n",
    "#subprocess.check_call([\"../scripts/sajjad.sh\"])\n",
    "#!ls -la\n",
    "#!echo \"Hello\"\n",
    "#!bash ../scripts/perform_r6.sh 10  ../convertdemo/dataset/rand_3.jpg ../logs/model_execution.log > ../logs/jupyter.log\n",
    "#myscript=\"../scripts/sajjad.sh\"\n",
    "#!bash {myscript}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62bb490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "['Create Neural Network' 'Verify Graph' 'Run the 1 time' 'Run the 2 time'\n",
      " 'Run the 3 time' 'Run the 4 time' 'Run the 5 time' 'Run the 6 time'\n",
      " 'Run the 7 time' 'Run the 8 time' 'Run the 9 time' 'Run the 10 time'\n",
      " 'Total   ' 'Average ']\n",
      "---------\n",
      "\n",
      "[' 14658us' ' 677us' ' 680.00us' ' 271.00us' ' 265.00us' ' 268.00us'\n",
      " ' 260.00us' ' 265.00us' ' 266.00us' ' 274.00us' ' 294.00us' ' 266.00us'\n",
      " ' 3147.00us' ' 314.70us']\n",
      "---------\n",
      "\n",
      "[14658.   677.   680.   271.   265.   268.   260.   265.   266.   274.\n",
      "   294.   266.  3147.   314.]\n",
      "---------\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "C0 = np.array(ProfileArray[0])\n",
    "C1 = np.array(ProfileArray[1])\n",
    "C2 = np.array(ProfileArray[2])\n",
    "print('---------\\n')\n",
    "print(C0)\n",
    "print('---------\\n')\n",
    "print(C1)\n",
    "print('---------\\n')\n",
    "print(C2)\n",
    "print('---------\\n')\n",
    "print(status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4628b805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "image = asarray(Image.open('../convertdemo/dataset/rand_6.jpg'))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "123a7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('../convertdemo/dataset/rand_6.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "043db9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ef91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
