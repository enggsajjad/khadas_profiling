{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f945bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from nni.algorithms.compression.pytorch.quantization import LsqQuantizer, QAT_Quantizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.numpy_helper\n",
    "from math import ceil\n",
    "### markus\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "from re import L\n",
    "import subprocess\n",
    "from subprocess import DEVNULL, STDOUT\n",
    "from xmlrpc.client import boolean\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "import ProfileClass as prof\n",
    "\n",
    "\n",
    "imgSizeW= 68#1280\n",
    "imgSizeH= 18#720\n",
    "gen_img_input_dim_w = imgSizeW\n",
    "gen_img_input_dim_h = imgSizeH\n",
    "gen_img_input_channels = 3\n",
    "\n",
    "# class Mnist(nn.Module):\n",
    "#     def __init__(self, input_channels, out_channels):\n",
    "#         super(Mnist, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels = input_channels, out_channels = out_channels, kernel_size = 5, stride = 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         return x\n",
    "\n",
    "# model = Mnist(gen_img_input_channels, 64)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# loop_run = '10'\n",
    "# MP = prof.ModelProfile()\n",
    "# [ProfileArray,status] = MP.doProfiling(model,\n",
    "#                  loop_run,\n",
    "#                  gen_img_input_channels,\n",
    "#                  gen_img_input_dim_h,\n",
    "#                  gen_img_input_dim_w,\n",
    "#                  debug=True)\n",
    "\n",
    "# accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSizeW= 68#1280\n",
    "\n",
    "imgSizeH= 18#720\n",
    "gen_img_input_dim_w = imgSizeW\n",
    "gen_img_input_dim_h = imgSizeH\n",
    "gen_img_input_channels = 3\n",
    "#test input image\n",
    "test_input_data=\"../convertdemo/dataset/rand_3.jpg\"\n",
    "#Paths\n",
    "quant_image_path = \"../quantization_images\"\n",
    "script_path = \"../scripts\"\n",
    "log_path = \"../logs\"\n",
    "network_path = \"../convertdemo/network\"\n",
    "#Files\n",
    "perform_script = \"perform_r6.sh\"\n",
    "parse_script = \"parse_r1.sh\"\n",
    "perform_log_file = \"model_execution.log\"\n",
    "parsed_log_file = \"model_execution_parsed.log\"\n",
    "model_name=\"mnist\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3c22",
   "metadata": {},
   "source": [
    "# Generate images based on some arbitrary input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = quant_image_path\n",
    "\n",
    "def generate_random_images(xdim, ydim, channels=3, count=1, path=\".\"):\n",
    "    \"\"\"\n",
    "    This functions generates random bmp images to use for quantization given\n",
    "    a defined dimension\n",
    "        @xdim   .. width of images\n",
    "        @ydim   .. height of images\n",
    "        @count  .. number of images\n",
    "        @path   .. path of images\n",
    "    \"\"\"\n",
    "\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(path)\n",
    "        print(\"The new directory quantization_images is created!\")\n",
    "\n",
    "    #delete the pre generated bmp/jpg files\n",
    "    filelist = [ f for f in os.listdir(quant_image_path) if (f.endswith(\".jpg\") or f.endswith(\".bmp\") ) ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(quant_image_path, f))\n",
    "        \n",
    "    for c in range(count):\n",
    "        rnd_img = np.random.randint(low=0,high=255, size=(ydim, xdim, channels),dtype=np.uint8) #imag.transpose((1,2,0)\n",
    "        imag_tp = np.ascontiguousarray(rnd_img, dtype=np.uint8)\n",
    "\n",
    "        pil_image = PIL.Image.frombytes('RGB',(xdim, ydim), imag_tp)\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".bmp\")\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "# choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## generate 10 random input images based on the provided dimensions\n",
    "generate_random_images(gen_img_input_dim_w, gen_img_input_dim_h, gen_img_input_channels, count=20, path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71df62",
   "metadata": {},
   "source": [
    "# Build the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef90da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from builder import *\n",
    "base_factor = 32\n",
    "class slice_reshape_operation(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2925dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slice_reshape_operation_2(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation_2, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op1 = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        self.op2 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op3 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op1(x)\n",
    "        x = self.op2(x)\n",
    "        x = self.op3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bbe32e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 12800])\n"
     ]
    }
   ],
   "source": [
    "op_name = \"ir_k3_re\"\n",
    "\n",
    "C_in = 4\n",
    "C_out = 4\n",
    "img_w = 10\n",
    "img_h = 10\n",
    "expansion = 6\n",
    "stride = 1\n",
    "\n",
    "#generate input to test\n",
    "input_x = torch.rand(1,3,base_factor*base_factor ,img_w*img_h*C_in*base_factor)\n",
    "\n",
    "print(input_x.shape)\n",
    "model = slice_reshape_operation_2(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "\n",
    "y = model(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041dad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3bd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(Mnist, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(num_channels, 20, 5, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels = input_channels, out_channels = out_channels, kernel_size = 5, stride = 1)\n",
    "        # self.conv2 = nn.Conv2d(20, 20, 5, 1)\n",
    "\n",
    "        # self.conv3 = nn.Conv2d(20, 50, 1, 1)\n",
    "\n",
    "        # f1 = int(imgSizeX/4) - 3\n",
    "        # f2 = int(imgSizeY/4) - 3\n",
    "        # self.fc1 = nn.Linear(f1*f2*50, 500)\n",
    "        # self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = F.max_pool2d(x, 2, 2)\n",
    "        # x = F.relu(self.conv2(x))\n",
    "\n",
    "        # x = F.relu(self.conv3(x))\n",
    "\n",
    "        # x = F.max_pool2d(x, 2, 2)\n",
    "        # f1 = int(imgSizeX/4) - 3\n",
    "        # f2 = int(imgSizeY/4) - 3\n",
    "        # x = x.view(-1, f1*f2*50)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aa64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(gen_img_input_dim_w*gen_img_input_dim_h, 50)#gen_img_input_dim_w*gen_img_input_dim_h\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # select first dim from [1,C,H,W] ie. [0,:,:,:]\n",
    "        x = x.select(0,0)\n",
    "        #print(x.shape)\n",
    "        # select first channel [0,:,:]\n",
    "        x = x.select(0,0)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape((1,-1))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SimpleNN()\n",
    "model = Mnist(gen_img_input_channels, 64)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d67f1",
   "metadata": {},
   "source": [
    "# Train the model for random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6de45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_images(count=1, path=\".\", extension='*.bmp'):\n",
    "    \"\"\"\n",
    "    loads images as np arrays; no normalization\n",
    "    \"\"\"\n",
    "    imgs =  []\n",
    "    files = glob.glob(path + \"/\" + extension)\n",
    "    for i, f in enumerate(files):\n",
    "        img = np.asarray(PIL.Image.open(f), dtype=np.float32).transpose((2,0,1))\n",
    "        img = img.reshape( (1,img.shape[0],img.shape[1], img.shape[2]) )\n",
    "        img = torch.from_numpy(img)\n",
    "        imgs.append(img)\n",
    "\n",
    "        if i+1 > count:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = load_images(count=2,path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f3c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for img in imgs:\n",
    "#     target = torch.tensor([2])\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(img)\n",
    "#     #print(output)\n",
    "#     loss = F.nll_loss(output, target)\n",
    "#     print(loss.item())\n",
    "#     #print(target, output)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "053c655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = slice_reshape_operation(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "model = slice_reshape_operation_2(op_name, C_in, C_out, img_w, img_h, expansion, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b567b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img_input_channels = 3\n",
    "gen_img_input_dim_h = base_factor*base_factor\n",
    "gen_img_input_dim_w = img_w*img_h*C_in*base_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "133ee91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ModelProfile Class...\n",
      "------------- Exporting to onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 3, 1024, 12800, strides=[39321600, 13107200, 12800, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_95 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_96 : Float(768, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_98 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_101 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_102 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_104 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_107 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_110 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_113 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_116 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_119 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_120 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_0\"](%onnx::Conv_102)\n",
      "  %onnx::Conv_117 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_1\"](%onnx::Conv_96)\n",
      "  %onnx::Conv_114 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_2\"](%onnx::Conv_96)\n",
      "  %onnx::Conv_111 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_3\"](%onnx::Conv_102)\n",
      "  %onnx::Conv_108 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_4\"](%onnx::Conv_96)\n",
      "  %onnx::Conv_105 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_5\"](%onnx::Conv_96)\n",
      "  %onnx::Conv_99 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_6\"](%onnx::Conv_96)\n",
      "  %onnx::Cast_55 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"Constant_7\"]() # /tmp/ipykernel_2994/2535447537.py:11:0\n",
      "  %onnx::Unsqueeze_121 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_8\"](%onnx::Cast_55) # /tmp/ipykernel_2994/2535447537.py:11:0\n",
      "  %onnx::Cast_56 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"Constant_9\"]() # /tmp/ipykernel_2994/2535447537.py:12:0\n",
      "  %onnx::Gather_122 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_10\"](%onnx::Cast_56) # /tmp/ipykernel_2994/2535447537.py:12:0\n",
      "  %onnx::Reshape_57 : Float(1, 1024, 12800, strides=[39321600, 12800, 1], requires_grad=0, device=cpu) = onnx::Gather[axis=1, onnx_name=\"Gather_11\"](%input, %onnx::Gather_122) # /tmp/ipykernel_2994/2535447537.py:12:0\n",
      "  %onnx::Cast_58 : Long(device=cpu) = onnx::Constant[value={128}, onnx_name=\"Constant_12\"]()\n",
      "  %onnx::Unsqueeze_123 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_13\"](%onnx::Cast_58)\n",
      "  %onnx::Cast_59 : Long(device=cpu) = onnx::Constant[value={320}, onnx_name=\"Constant_14\"]()\n",
      "  %onnx::Unsqueeze_124 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_15\"](%onnx::Cast_59)\n",
      "  %onnx::Cast_60 : Long(device=cpu) = onnx::Constant[value={320}, onnx_name=\"Constant_16\"]()\n",
      "  %onnx::Unsqueeze_125 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_17\"](%onnx::Cast_60)\n",
      "  %onnx::Concat_61 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_18\"](%onnx::Unsqueeze_121)\n",
      "  %onnx::Concat_62 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_19\"](%onnx::Unsqueeze_123)\n",
      "  %onnx::Concat_63 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_20\"](%onnx::Unsqueeze_124)\n",
      "  %onnx::Concat_64 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_21\"](%onnx::Unsqueeze_125)\n",
      "  %onnx::Reshape_65 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"Concat_22\"](%onnx::Concat_61, %onnx::Concat_62, %onnx::Concat_63, %onnx::Concat_64) # /tmp/ipykernel_2994/2535447537.py:14:0\n",
      "  %input.1 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"Reshape_23\"](%onnx::Reshape_57, %onnx::Reshape_65) # /tmp/ipykernel_2994/2535447537.py:14:0\n",
      "  %input.7 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_24\"](%input.1, %onnx::Conv_95, %onnx::Conv_96) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_69 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_25\"](%input.7) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.15 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_26\"](%onnx::Conv_69, %onnx::Conv_98, %onnx::Conv_99) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_72 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_27\"](%input.15) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_100 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_28\"](%onnx::Conv_72, %onnx::Conv_101, %onnx::Conv_102) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_75 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_29\"](%onnx::Add_100, %input.1) # /home/sajjad/sajjad/scripts/notebook/builder.py:295:0\n",
      "  %input.27 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_30\"](%onnx::Conv_75, %onnx::Conv_104, %onnx::Conv_105) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_78 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_31\"](%input.27) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.35 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_32\"](%onnx::Conv_78, %onnx::Conv_107, %onnx::Conv_108) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_81 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_33\"](%input.35) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_109 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_34\"](%onnx::Conv_81, %onnx::Conv_110, %onnx::Conv_111) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_84 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_35\"](%onnx::Add_109, %onnx::Conv_75) # /home/sajjad/sajjad/scripts/notebook/builder.py:295:0\n",
      "  %input.47 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_36\"](%onnx::Conv_84, %onnx::Conv_113, %onnx::Conv_114) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_87 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_37\"](%input.47) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.55 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_38\"](%onnx::Conv_87, %onnx::Conv_116, %onnx::Conv_117) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %onnx::Conv_90 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_39\"](%input.55) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_118 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_40\"](%onnx::Conv_90, %onnx::Conv_119, %onnx::Conv_120) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453:0\n",
      "  %output : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_41\"](%onnx::Add_118, %onnx::Conv_84) # /home/sajjad/sajjad/scripts/notebook/builder.py:295:0\n",
      "  return (%output)\n",
      "\n",
      "------------- Checking exported model\n",
      "------------- Performing the profiling...\n",
      "\u001b[35m>>>>> Setting Environment Variables Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Entering convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "2022-10-13 16:46:47.161234: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:46:47.161261: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Start importing onnx...\u001b[0m\n",
      "\u001b[32mI Current ONNX Model use ir_version 3 opset_version 7\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'eliminate_option_const' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_const_branch' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_if' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_sequence_construct_concat_from_sequence' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_lrn_lowlevel_implement' success\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_116 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_110 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_104 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_113 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_119 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_102 shape: [128]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_101 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_96 shape: [768]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_98 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Constant_onnx::Reshape_65 shape: [4]\u001b[0m\n",
      "\u001b[36mD Calc tensor Constant_onnx::Gather_122 shape: []\u001b[0m\n",
      "\u001b[36mD Calc tensor Gather_onnx::Reshape_57 shape: [1, 1024, 12800]\u001b[0m\n",
      "\u001b[36mD Calc tensor Reshape_input.1 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_95 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.7 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_69 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_107 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.15 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_72 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_100 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_75 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.27 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_78 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.35 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_81 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_109 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_84 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.47 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_87 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.55 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_90 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_118 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_output shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[32mI build output layer attach_Add_Add_41:out0\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_41:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_41']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_40:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_40', 'Initializer_onnx::Conv_102', 'Initializer_onnx::Conv_119']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_35:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_35']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_39:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_39']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_34:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_34', 'Initializer_onnx::Conv_102', 'Initializer_onnx::Conv_110']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_29:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_29']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_38:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_38', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_116']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_33:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_33']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_28:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_28', 'Initializer_onnx::Conv_102', 'Initializer_onnx::Conv_101']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Reshape_Reshape_23:out0\u001b[0m\n",
      "\u001b[32mI Match r_rsp_v5 [['Reshape_Reshape_23', 'Constant_Concat_22_onnx__Reshape_65_as_const']] [['Reshape', 'Constant_0']] to [['reshape']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_37:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_37']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_32:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_32', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_107']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_27:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_27']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Gather_Gather_11:out0\u001b[0m\n",
      "\u001b[32mI Match r_gather [['Gather_Gather_11']] [['Gather']] to [['gather']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_36:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_36', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_113']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_31:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_31']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_26:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_26', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_98']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_25:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_25']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Constant_Cast_10_onnx__Gather_122_as_const:out0\u001b[0m\n",
      "\u001b[32mI Match r_variable [['Constant_Cast_10_onnx__Gather_122_as_const']] [['Constant']] to [['variable']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_24:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_24', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_95']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_30:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Conv_Conv_30', 'Initializer_onnx::Conv_96', 'Initializer_onnx::Conv_104']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI build input layer input:out0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_40_2 0  ~ Add_Add_41_1 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_35_3 0  ~ Add_Add_41_1 1\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_39_4 0  ~ Conv_Conv_40_2 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_34_5 0  ~ Add_Add_35_3 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_29_6 0  ~ Add_Add_35_3 1\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_38_7 0  ~ Relu_Relu_39_4 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_33_8 0  ~ Conv_Conv_34_5 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_28_9 0  ~ Add_Add_29_6 0\u001b[0m\n",
      "\u001b[36mD connect Reshape_Reshape_23_10 0  ~ Add_Add_29_6 1\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_37_11 0  ~ Conv_Conv_38_7 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_32_12 0  ~ Relu_Relu_33_8 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_27_13 0  ~ Conv_Conv_28_9 0\u001b[0m\n",
      "\u001b[36mD connect Gather_Gather_11_14 0  ~ Reshape_Reshape_23_10 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_36_15 0  ~ Relu_Relu_37_11 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_31_16 0  ~ Conv_Conv_32_12 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_26_17 0  ~ Relu_Relu_27_13 0\u001b[0m\n",
      "\u001b[36mD connect input_22 0  ~ Gather_Gather_11_14 0\u001b[0m\n",
      "\u001b[36mD connect Constant_Cast_10_onnx__Gather_122_as_const_19 0  ~ Gather_Gather_11_14 1\u001b[0m\n",
      "\u001b[36mD connect Add_Add_35_3 0  ~ Conv_Conv_36_15 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_30_21 0  ~ Relu_Relu_31_16 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_25_18 0  ~ Conv_Conv_26_17 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_24_20 0  ~ Relu_Relu_25_18 0\u001b[0m\n",
      "\u001b[36mD connect Reshape_Reshape_23_10 0  ~ Conv_Conv_24_20 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_29_6 0  ~ Conv_Conv_30_21 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_41_1 0  ~ attach_Add_Add_41/out0_0 0\u001b[0m\n",
      "2022-10-13 16:46:50.105206: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 16:46:50.133104: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793010000 Hz\n",
      "2022-10-13 16:46:50.133667: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f17aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-13 16:46:50.133700: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-13 16:46:50.136174: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:46:50.136201: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-13 16:46:50.136237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (14212ee128a7): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36mD Process input_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_22:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_23_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_23_10:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_25_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_25_18:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_26_17:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_27_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_27_13:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_28_9:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_31_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_31_16:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_32_12:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_33_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_33_8:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_34_5:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_37_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_37_11:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_38_7:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_39_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_39_4:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_40_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_41/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_41/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Start C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Optimizing network with broadcast_op\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_41_1_acuity_mark_perm_23 before Add_Add_41_1\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_36_15_acuity_mark_perm_24 before Conv_Conv_36_15\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_35_3_acuity_mark_perm_25 before Add_Add_35_3\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_30_21_acuity_mark_perm_26 before Conv_Conv_30_21\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_29_6_acuity_mark_perm_27 before Add_Add_29_6\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_24_20_acuity_mark_perm_28 before Conv_Conv_24_20\u001b[0m\n",
      "\u001b[32mI End C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Process input_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_22:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_23_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_23_10:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20_acuity_mark_perm_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20_acuity_mark_perm_28:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_25_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_25_18:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_26_17:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_27_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_27_13:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_28_9:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6_acuity_mark_perm_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6_acuity_mark_perm_27:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21_acuity_mark_perm_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21_acuity_mark_perm_26:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_31_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_31_16:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_32_12:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_33_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_33_8:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_34_5:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3_acuity_mark_perm_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3_acuity_mark_perm_25:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15_acuity_mark_perm_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15_acuity_mark_perm_24:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_37_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_37_11:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_38_7:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_39_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_39_4:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_40_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1_acuity_mark_perm_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1_acuity_mark_perm_23:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_41/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_41/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[36mD Optimizing network with force_1d_tensor, swapper, merge_duplicate_quantize_dequantize, merge_layer, auto_fill_bn, auto_fill_l2normalizescale, auto_fill_instancenormalize, resize_nearest_transformer, auto_fill_multiply, compute_gather_negative, auto_fill_zero_bias, proposal_opt_import, special_add_to_conv2d, extend_gather_to_gather_reshape\u001b[0m\n",
      "\u001b[32mI End importing onnx...\u001b[0m\n",
      "\u001b[32mI Dump net to mnist.json\u001b[0m\n",
      "\u001b[32mI Save net to mnist.data\u001b[0m\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "-------------------- QUANTIZATION SCRIPT\n",
      "2022-10-13 16:46:50.768201: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:46:50.768231: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Namespace(action='quantization', batch_size=1, caffe_mean_file=None, capture_format='nchw', capture_quantized=False, channel_mean_value='127 127 127 255', config=None, data_output=None, debug=True, device=None, divergence_first_quantize_bits=11, dtype='float', epochs=1, epochs_per_decay=100, force_gray=False, fpfs_delta0=1, fpfs_epochs=0, fpfs_reduce_target=0, input_fitting='scale', input_normalization=None, lr=0.1, mean_file=None, model_data='mnist.data', model_data_format='zone', model_input='mnist.json', model_quantize=None, optimizer='momentum', output_dir=None, output_num=5, pb_name=None, pfps_delta0=1, pfps_epochs=0, pfps_reduce_target=0, prune_epochs=10, prune_loss=1, quantized_algorithm='normal', quantized_divergence_nbins=0, quantized_dtype='asymmetric_affine-u8', quantized_hybrid=False, quantized_moving_alpha=0.0, quantized_rebuild=False, quantized_rebuild_all=True, random_brightness=None, random_contrast=None, random_crop=False, random_flip=False, random_mirror=False, reorder_channel=None, restart=False, samples=-1, source='text', source_file='dataset/dataset1.txt', task='classification', validation_output='validation.csv', without_update_masked_grad=False)\u001b[0m\n",
      "2022-10-13 16:46:52.134922: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 16:46:52.161131: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793010000 Hz\n",
      "2022-10-13 16:46:52.161816: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5214860 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-13 16:46:52.161846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-13 16:46:52.164106: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:46:52.164130: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-13 16:46:52.164151: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (14212ee128a7): /proc/driver/nvidia/version does not exist\n",
      "\u001b[32mI Load model in mnist.json\u001b[0m\n",
      "\u001b[32mI Load data in mnist.data\u001b[0m\n",
      "W:tensorflow:From acuitylib/app/tensorzone/workspace.py:26: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "\u001b[32mI Fitting image with scale.\u001b[0m\n",
      "\u001b[32mI Channel mean value [127.0, 127.0, 127.0, 255.0]\u001b[0m\n",
      "\u001b[32mI [TRAINER]Quantization start...\u001b[0m\n",
      "[TRAINER]Quantization start...\n",
      "\u001b[32mI Init validate tensor provider.\u001b[0m\n",
      "\u001b[32mI Enqueue samples 20\u001b[0m\n",
      "\u001b[32mI Init provider with 20 samples.\u001b[0m\n",
      "\u001b[36mD set up a quantize net\u001b[0m\n",
      "\u001b[36mD Process input_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 3, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Real output shape: (1,)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 1, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14_acuity_opt_gather_reshape_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14_acuity_opt_gather_reshape_29:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_23_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_23_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20_acuity_mark_perm_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20_acuity_mark_perm_28:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_25_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_25_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_26_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_27_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_27_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_28_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6_acuity_mark_perm_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6_acuity_mark_perm_27:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21_acuity_mark_perm_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21_acuity_mark_perm_26:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_31_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_31_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_32_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_33_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_33_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_34_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3_acuity_mark_perm_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3_acuity_mark_perm_25:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15_acuity_mark_perm_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15_acuity_mark_perm_24:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_37_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_37_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_38_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_39_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_39_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_40_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1_acuity_mark_perm_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1_acuity_mark_perm_23:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_41/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_41/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Generated network graph with 1 outputs.\u001b[0m\n",
      "\u001b[32mI  @attach_Add_Add_41/out0_0:out0: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Init coefficients ...\u001b[0m\n",
      "\u001b[32mI Start tensor porvider ... \u001b[0m\n",
      "\u001b[32mI Runing 1 epochs, algorithm: normal\u001b[0m\n",
      "\u001b[32mI iterations: 0\u001b[0m\n",
      "\u001b[36mD Quantize tensor @attach_Add_Add_41/out0_0:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_41_1:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_40_2:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_35_3:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_39_4:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_34_5:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_29_6:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_38_7:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_33_8:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_28_9:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Reshape_Reshape_23_10:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_37_11:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_32_12:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_27_13:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Gather_Gather_11_14:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_36_15:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_31_16:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_26_17:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_25_18:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Constant_Cast_10_onnx__Gather_122_as_const_19:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_24_20:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_30_21:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @input_22:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_41_1_acuity_mark_perm_23:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_36_15_acuity_mark_perm_24:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_35_3_acuity_mark_perm_25:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_30_21_acuity_mark_perm_26:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_29_6_acuity_mark_perm_27:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_24_20_acuity_mark_perm_28:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Gather_Gather_11_14_acuity_opt_gather_reshape_29:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_40_2:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_34_5:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_38_7:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_28_9:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_32_12:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_36_15:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_26_17:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_24_20:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_30_21:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_40_2:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_34_5:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_38_7:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_28_9:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_32_12:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_36_15:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_26_17:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_24_20:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_30_21:bias.\u001b[0m\n",
      "\u001b[32mI Clean.\u001b[0m\n",
      "\u001b[36mD Optimizing network with align_quantize, broadcast_quantize, qnt_adjust_coef, qnt_adjust_param\u001b[0m\n",
      "\u001b[32mI Dump net quantize tensor table to mnist.quantize\u001b[0m\n",
      "\u001b[32mI [TRAINER]Quantization complete.\u001b[0m\n",
      "[TRAINER]Quantization complete.\n",
      "\u001b[32mI Save net to mnist.data\u001b[0m\n",
      "\u001b[32mI Clean.\u001b[0m\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "-------------------- EXPORT CASE\n",
      "2022-10-13 16:46:58.296350: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:46:58.296384: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Load model in mnist.json\u001b[0m\n",
      "\u001b[32mI Load data in mnist.data\u001b[0m\n",
      "\u001b[32mI Load quantization tensor table mnist.quantize\u001b[0m\n",
      "2022-10-13 16:47:00.017966: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 16:47:00.041118: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793010000 Hz\n",
      "2022-10-13 16:47:00.041700: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57398d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-13 16:47:00.041729: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-13 16:47:00.043809: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-13 16:47:00.043837: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-13 16:47:00.043859: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (14212ee128a7): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36mD Process input_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14_acuity_opt_gather_reshape_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14_acuity_opt_gather_reshape_29:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_23_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_23_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20_acuity_mark_perm_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20_acuity_mark_perm_28:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_25_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_25_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_26_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_27_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_27_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_28_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6_acuity_mark_perm_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6_acuity_mark_perm_27:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21_acuity_mark_perm_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21_acuity_mark_perm_26:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_31_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_31_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_32_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_33_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_33_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_34_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3_acuity_mark_perm_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3_acuity_mark_perm_25:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15_acuity_mark_perm_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15_acuity_mark_perm_24:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_37_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_37_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_38_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_39_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_39_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_40_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1_acuity_mark_perm_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1_acuity_mark_perm_23:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_41/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_41/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Initialzing network optimizer by /acuity-toolkit/convertdemo/../bin/VIPNANOQI_PID0X88 ...\u001b[0m\n",
      "\u001b[36mD Optimizing network with merge_ximum, qnt_adjust_coef, multiply_transform, add_extra_io, format_input_ops, auto_fill_zero_bias, conv_kernel_transform, strip_op, extend_unstack_split, merge_layer, transform_layer, broadcast_op, strip_op, auto_fill_reshape_zero, adjust_output_attrs, insert_dtype_converter\u001b[0m\n",
      "\u001b[36mD Strip layer Gather_Gather_11_14_acuity_opt_gather_reshape_29(reshape)\u001b[0m\n",
      "\u001b[36mD Insert dtype_converter Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14 between Constant_Cast_10_onnx__Gather_122_as_const_19 and Gather_Gather_11_14\u001b[0m\n",
      "\u001b[32mI Start T2C Switcher...\u001b[0m\n",
      "\u001b[36mD Optimizing network with broadcast_op, t2c_fc\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_41_1_acuity_mark_perm_23_acuity_mark_perm_30 before Add_Add_41_1_acuity_mark_perm_23\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_35_3_acuity_mark_perm_25_acuity_mark_perm_31 before Add_Add_35_3_acuity_mark_perm_25\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_29_6_acuity_mark_perm_27_acuity_mark_perm_32 before Add_Add_29_6_acuity_mark_perm_27\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_36_15_acuity_mark_perm_33 before Conv_Conv_36_15\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_30_21_acuity_mark_perm_34 before Conv_Conv_30_21\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_24_20_acuity_mark_perm_35 before Conv_Conv_24_20\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_24_20_acuity_mark_perm_28\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_24_20_acuity_mark_perm_35\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_29_6_acuity_mark_perm_27_acuity_mark_perm_32\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_29_6_acuity_mark_perm_27\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_30_21_acuity_mark_perm_26\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_30_21_acuity_mark_perm_34\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_35_3_acuity_mark_perm_25_acuity_mark_perm_31\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_35_3_acuity_mark_perm_25\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_36_15_acuity_mark_perm_24\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_36_15_acuity_mark_perm_33\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_41_1_acuity_mark_perm_23_acuity_mark_perm_30\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_41_1_acuity_mark_perm_23\u001b[0m\n",
      "\u001b[32mI End T2C Switcher...\u001b[0m\n",
      "\u001b[36mD Process input_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(dtype_converter): (1)\u001b[0m\n",
      "\u001b[36mD Tensor @Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14:out0 type: int64\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_11_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_11_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_23_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_23_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_24_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_25_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_25_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_26_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_27_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_27_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_28_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_29_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_29_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_30_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_31_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_31_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_32_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_33_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_33_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_34_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_35_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_35_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_36_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_37_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_37_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_38_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_39_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_39_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_40_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_41_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_41_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_41/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_41/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[36mD Optimizing network with conv_1xn_transform, proposal_opt, c2drv_convert_axis, c2drv_convert_shape, c2drv_convert_array, c2drv_cast_dtype, c2drv_trans_data\u001b[0m\n",
      "\u001b[32mI Building data ...\u001b[0m\n",
      "\u001b[32mI Packing data ...\u001b[0m\n",
      "\u001b[36mD Packing Constant_Cast_10_onnx__Gather_122_as_const_19 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Constant_Cast_10_onnx__Gather_122_as_const_19:data to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_24_20 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_24_20:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_24_20:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_26_17 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_26_17:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_26_17:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_28_9 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_28_9:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_28_9:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_30_21 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_30_21:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_30_21:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_32_12 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_32_12:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_32_12:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_34_5 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_34_5:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_34_5:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_36_15 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_36_15:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_36_15:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_38_7 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_38_7:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_38_7:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_40_2 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_40_2:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_40_2:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[32mI Saving data to mnist.export.data\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_mnist.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_mnist.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_post_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_post_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_pre_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_pre_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_global.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/main.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/BUILD\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/mnist.vcxproj\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/makefile.linux\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/.cproject\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/.project\u001b[0m\n",
      "\u001b[36mD Generate fake input /acuity-toolkit/convertdemo/input_22_0.tensor\u001b[0m\n",
      "mv: '/acuity-toolkit/convertdemo/network_binary.nb' and '/acuity-toolkit/convertdemo/network_binary.nb' are the same file\n",
      "mv: '/acuity-toolkit/convertdemo/input_0.dat' and '/acuity-toolkit/convertdemo/input_0.dat' are the same file\n",
      "mv: '/acuity-toolkit/convertdemo/output0_320_320_128_1.dat' and '/acuity-toolkit/convertdemo/output0_320_320_128_1.dat' are the same file\n",
      "\u001b[32mI Dump nbg input meta to /acuity-toolkit/convertdemo_nbg_unify/nbg_meta.json\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_mnist.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_mnist.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_post_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_post_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_pre_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_pre_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_global.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/main.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/BUILD\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/mnist.vcxproj\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/makefile.linux\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/.cproject\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/.project\u001b[0m\n",
      "/acuity-toolkit/convertdemo_nbg_unify\n",
      "customer:input,0,1:output,0,0:\n",
      "*********************************\n",
      "/acuity-toolkit/convertdemo\n",
      "/\n",
      "/acuity-toolkit/convertdemo_nbg_unify\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "\u001b[35m>>>>> Exiting convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "patching file main.c\n",
      "\u001b[35m>>>>> Patching main.c on Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_pre_process.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_mnist.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vnn_mnist.c: In function vnn_CreateMnist:\n",
      "vnn_mnist.c:146:29: warning: unused variable data [-Wunused-variable]\n",
      "  146 |     uint8_t *               data;\n",
      "      |                             ^~~~\n",
      "At top level:\n",
      "vnn_mnist.c:94:17: warning: load_data defined but not used [-Wunused-function]\n",
      "   94 | static uint8_t* load_data\n",
      "      |                 ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COMPILE /home/khadas/nbg_unify_mnist/main.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_post_process.c\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing network on Khadas is saved in model_execution.log <<<<<<<\u001b[37m\n",
      "------------- Performing the profiling done!\n",
      "------------- Parsing the profiling results...\n",
      "Parsing ../logs/model_execution.log....\n",
      "------------- Parsing the profiling results done!\n",
      "------------- auto_profile done!...\n"
     ]
    }
   ],
   "source": [
    "loop_run = '10'\n",
    "MP = prof.ModelProfile()\n",
    "[ProfileArray,status] = MP.doProfiling(model,\n",
    "                 loop_run,\n",
    "                 gen_img_input_channels,\n",
    "                 gen_img_input_dim_h,\n",
    "                 gen_img_input_dim_w,\n",
    "                 debug=True)\n",
    "\n",
    "accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec76fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_call(['../scripts/sajjad.sh'])\n",
    "#subprocess.check_call([\"../scripts/sajjad.sh\"])\n",
    "#!ls -la\n",
    "#!echo \"Hello\"\n",
    "#!bash ../scripts/perform_r6.sh 10  ../convertdemo/dataset/rand_3.jpg ../logs/model_execution.log > ../logs/jupyter.log\n",
    "#myscript=\"../scripts/sajjad.sh\"\n",
    "#!bash {myscript}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run#1 4 cin 4cout number 1\n",
    "\n",
    "['1365544' '2695579' '272934' '271745' '271674' '271654' '272228' '271578'\n",
    " '271715' '274609' '272346' '271730' '2722444' '272244']\n",
    "\n",
    "\n",
    "Run#2 4 cin 4cout number 2\n",
    "\n",
    "['97151' '2356437' '351619' '353184' '351332' '351130' '351330' '354675'\n",
    " '351358' '351234' '351391' '351176' '3518616' '351861']\n",
    "Run#3 4 cin 4cout number 3\n",
    "['84047' '38567' '524027' '522490' '522432' '522506' '523456' '522375'\n",
    " '522196' '522294' '523748' '522288' '5228013' '522801']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62bb490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "['Create Neural Network' 'Verify Graph' 'Run the 1 time' 'Run the 2 time'\n",
      " 'Run the 3 time' 'Run the 4 time' 'Run the 5 time' 'Run the 6 time'\n",
      " 'Run the 7 time' 'Run the 8 time' 'Run the 9 time' 'Run the 10 time'\n",
      " 'Total   ' 'Average ']\n",
      "---------\n",
      "\n",
      "[' 84047us' ' 38567us' ' 524027.00us' ' 522490.00us' ' 522432.00us'\n",
      " ' 522506.00us' ' 523456.00us' ' 522375.00us' ' 522196.00us'\n",
      " ' 522294.00us' ' 523748.00us' ' 522288.00us' ' 5228013.00us'\n",
      " ' 522801.31us']\n",
      "---------\n",
      "\n",
      "['84047' '38567' '524027' '522490' '522432' '522506' '523456' '522375'\n",
      " '522196' '522294' '523748' '522288' '5228013' '522801']\n",
      "---------\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "C0 = np.array(ProfileArray[0])\n",
    "C1 = np.array(ProfileArray[1])\n",
    "C2 = np.array(ProfileArray[2])\n",
    "print('---------\\n')\n",
    "print(C0)\n",
    "print('---------\\n')\n",
    "print(C1)\n",
    "print('---------\\n')\n",
    "print(C2)\n",
    "print('---------\\n')\n",
    "print(status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "image = asarray(Image.open('../convertdemo/dataset/rand_6.jpg'))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('../convertdemo/dataset/rand_6.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043db9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef91a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
