{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f945bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from nni.algorithms.compression.pytorch.quantization import LsqQuantizer, QAT_Quantizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.numpy_helper\n",
    "from math import ceil\n",
    "### markus\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "from re import L\n",
    "import subprocess\n",
    "from subprocess import DEVNULL, STDOUT\n",
    "from xmlrpc.client import boolean\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "import ProfileClass as prof\n",
    "\n",
    "\n",
    "# imgSizeW= 68#1280\n",
    "# imgSizeH= 18#720\n",
    "# gen_img_input_dim_w = imgSizeW\n",
    "# gen_img_input_dim_h = imgSizeH\n",
    "# gen_img_input_channels = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgSizeW= 68#1280\n",
    "\n",
    "# imgSizeH= 18#720\n",
    "# gen_img_input_dim_w = imgSizeW\n",
    "# gen_img_input_dim_h = imgSizeH\n",
    "# gen_img_input_channels = 3\n",
    "# #test input image\n",
    "# test_input_data=\"../convertdemo/dataset/rand_3.jpg\"\n",
    "# #Paths\n",
    "# quant_image_path = \"../quantization_images\"\n",
    "# script_path = \"../scripts\"\n",
    "# log_path = \"../logs\"\n",
    "# network_path = \"../convertdemo/network\"\n",
    "# #Files\n",
    "# perform_script = \"perform_r6.sh\"\n",
    "# parse_script = \"parse_r1.sh\"\n",
    "# perform_log_file = \"model_execution.log\"\n",
    "# parsed_log_file = \"model_execution_parsed.log\"\n",
    "# model_name=\"mnist\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71df62",
   "metadata": {},
   "source": [
    "# Build the normal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4be745",
   "metadata": {},
   "source": [
    "### Example Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef90da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from builder import *\n",
    "base_factor = 32\n",
    "class slice_reshape_operation(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca95e3",
   "metadata": {},
   "source": [
    "### Example Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2925dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slice_reshape_operation_2(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation_2, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op1 = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        self.op2 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op3 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op1(x)\n",
    "        x = self.op2(x)\n",
    "        x = self.op3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2bf7a",
   "metadata": {},
   "source": [
    "### Example Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f5d6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slice_reshape_operation_3(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation_3, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op1 = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        self.op2 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op3 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op4 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op1(x)\n",
    "        x = self.op2(x)\n",
    "        x = self.op3(x)\n",
    "        x = self.op4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cef8d3",
   "metadata": {},
   "source": [
    "### Example Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a81a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slice_reshape_operation_4(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation_4, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op1 = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        self.op2 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op3 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op4 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op5 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op1(x)\n",
    "        x = self.op2(x)\n",
    "        x = self.op3(x)\n",
    "        x = self.op4(x)\n",
    "        x = self.op5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53396c6c",
   "metadata": {},
   "source": [
    "### Example Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e01a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slice_reshape_operation_5(nn.Module):\n",
    "    def __init__(self, op_name, C_in, C_out, img_w, img_h, expansion, stride):\n",
    "        super(slice_reshape_operation_5, self).__init__()\n",
    "        self.img_w = img_w*base_factor\n",
    "        self.img_h = img_h*base_factor\n",
    "        self.C_in = C_in*base_factor\n",
    "        self.op1 = PRIMITIVES[op_name](self.C_in, C_out*base_factor, expansion, stride)\n",
    "        self.op2 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op3 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op4 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op5 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "        self.op6 = PRIMITIVES[op_name](C_out*base_factor, C_out*base_factor, expansion, stride)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.select(1,0)\n",
    "        #x = x.select(2,0)\n",
    "        x = torch.reshape(x,(batch_size, self.C_in,  self.img_h, self.img_w))\n",
    "        #x = x.reshape(batch_size, self.C_in,  self.img_h, self.img_w)\n",
    "        x = self.op1(x)\n",
    "        x = self.op2(x)\n",
    "        x = self.op3(x)\n",
    "        x = self.op4(x)\n",
    "        x = self.op5(x)\n",
    "        x = self.op6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4bbe32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_name = \"ir_k3_re\"\n",
    "\n",
    "C_in = 4\n",
    "C_out = 4\n",
    "img_w = 10\n",
    "img_h = 10\n",
    "expansion = 6\n",
    "stride = 1\n",
    "\n",
    "#generate input to test\n",
    "input_x = torch.rand(1,3,base_factor*base_factor ,img_w*img_h*C_in*base_factor)\n",
    "\n",
    "# print(input_x.shape)\n",
    "# model = slice_reshape_operation_2(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "\n",
    "# y = model(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "053c655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = slice_reshape_operation(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "#model = slice_reshape_operation_2(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "#model = slice_reshape_operation_3(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "#model = slice_reshape_operation_4(op_name, C_in, C_out, img_w, img_h, expansion, stride)\n",
    "model = slice_reshape_operation_5(op_name, C_in, C_out, img_w, img_h, expansion, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92bfe6",
   "metadata": {},
   "source": [
    "# Call the profile function\n",
    "the class is:\n",
    "- import ProfileClass as prof\n",
    "- MP = prof.ModelProfile()\n",
    "\n",
    "the function is:\n",
    "\n",
    "- doProfiling(model, loop_run, gen_img_input_channels, gen_img_input_dim_h, gen_img_input_dim_w, debug=True) \n",
    "\n",
    "the parameters are:\n",
    "- the model to be profiled\n",
    "- number of runs to be profiled\n",
    "- channels for images\n",
    "- image height\n",
    "- image width\n",
    "- debugging enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b567b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img_input_channels = 3\n",
    "gen_img_input_dim_h = base_factor*base_factor\n",
    "gen_img_input_dim_w = img_w*img_h*C_in*base_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "133ee91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ModelProfile Class...\n",
      "------------- Exporting to onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n",
      "Warning: Constant folding - unsupported opset version. Constant folding not applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 3, 1024, 12800, strides=[39321600, 13107200, 12800, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_176 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_177 : Float(768, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_179 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_182 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_183 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_185 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_188 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_191 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_194 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_197 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_200 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_203 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_206 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_209 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_212 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_215 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_218 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_221 : Float(768, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_224 : Float(768, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_227 : Float(128, 768, 1, 1, strides=[768, 1, 1, 1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_228 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_0\"](%onnx::Conv_183)\n",
      "  %onnx::Conv_225 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_1\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_222 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_2\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_219 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_3\"](%onnx::Conv_183)\n",
      "  %onnx::Conv_216 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_4\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_213 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_5\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_210 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_6\"](%onnx::Conv_183)\n",
      "  %onnx::Conv_207 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_7\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_204 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_8\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_201 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_9\"](%onnx::Conv_183)\n",
      "  %onnx::Conv_198 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_10\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_195 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_11\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_192 : Float(128, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_12\"](%onnx::Conv_183)\n",
      "  %onnx::Conv_189 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_13\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_186 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_14\"](%onnx::Conv_177)\n",
      "  %onnx::Conv_180 : Float(768, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_15\"](%onnx::Conv_177)\n",
      "  %onnx::Cast_109 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"Constant_16\"]() # /tmp/ipykernel_42016/1616587714.py:14:0\n",
      "  %onnx::Unsqueeze_229 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_17\"](%onnx::Cast_109) # /tmp/ipykernel_42016/1616587714.py:14:0\n",
      "  %onnx::Cast_110 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"Constant_18\"]() # /tmp/ipykernel_42016/1616587714.py:14:0\n",
      "  %onnx::Gather_230 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_19\"](%onnx::Cast_110) # /tmp/ipykernel_42016/1616587714.py:14:0\n",
      "  %onnx::Reshape_111 : Float(1, 1024, 12800, strides=[39321600, 12800, 1], requires_grad=0, device=cpu) = onnx::Gather[axis=1, onnx_name=\"Gather_20\"](%input, %onnx::Gather_230) # /tmp/ipykernel_42016/1616587714.py:14:0\n",
      "  %onnx::Cast_112 : Long(device=cpu) = onnx::Constant[value={128}, onnx_name=\"Constant_21\"]()\n",
      "  %onnx::Unsqueeze_231 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_22\"](%onnx::Cast_112)\n",
      "  %onnx::Cast_113 : Long(device=cpu) = onnx::Constant[value={320}, onnx_name=\"Constant_23\"]()\n",
      "  %onnx::Unsqueeze_232 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_24\"](%onnx::Cast_113)\n",
      "  %onnx::Cast_114 : Long(device=cpu) = onnx::Constant[value={320}, onnx_name=\"Constant_25\"]()\n",
      "  %onnx::Unsqueeze_233 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_26\"](%onnx::Cast_114)\n",
      "  %onnx::Concat_115 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_27\"](%onnx::Unsqueeze_229)\n",
      "  %onnx::Concat_116 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_28\"](%onnx::Unsqueeze_231)\n",
      "  %onnx::Concat_117 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_29\"](%onnx::Unsqueeze_232)\n",
      "  %onnx::Concat_118 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[axes=[0], onnx_name=\"Unsqueeze_30\"](%onnx::Unsqueeze_233)\n",
      "  %onnx::Reshape_119 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"Concat_31\"](%onnx::Concat_115, %onnx::Concat_116, %onnx::Concat_117, %onnx::Concat_118) # /tmp/ipykernel_42016/1616587714.py:17:0\n",
      "  %input.1 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"Reshape_32\"](%onnx::Reshape_111, %onnx::Reshape_119) # /tmp/ipykernel_42016/1616587714.py:17:0\n",
      "  %input.7 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_33\"](%input.1, %onnx::Conv_176, %onnx::Conv_177) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_123 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_34\"](%input.7) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.15 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_35\"](%onnx::Conv_123, %onnx::Conv_179, %onnx::Conv_180) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_126 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_36\"](%input.15) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_181 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_37\"](%onnx::Conv_126, %onnx::Conv_182, %onnx::Conv_183) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_129 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_38\"](%onnx::Add_181, %input.1) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  %input.27 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_39\"](%onnx::Conv_129, %onnx::Conv_185, %onnx::Conv_186) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_132 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_40\"](%input.27) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.35 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_41\"](%onnx::Conv_132, %onnx::Conv_188, %onnx::Conv_189) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_135 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_42\"](%input.35) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_190 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_43\"](%onnx::Conv_135, %onnx::Conv_191, %onnx::Conv_192) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_138 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_44\"](%onnx::Add_190, %onnx::Conv_129) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  %input.47 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_45\"](%onnx::Conv_138, %onnx::Conv_194, %onnx::Conv_195) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_141 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_46\"](%input.47) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.55 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_47\"](%onnx::Conv_141, %onnx::Conv_197, %onnx::Conv_198) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_144 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_48\"](%input.55) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_199 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_49\"](%onnx::Conv_144, %onnx::Conv_200, %onnx::Conv_201) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_147 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_50\"](%onnx::Add_199, %onnx::Conv_138) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  %input.67 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_51\"](%onnx::Conv_147, %onnx::Conv_203, %onnx::Conv_204) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_150 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_52\"](%input.67) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.75 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_53\"](%onnx::Conv_150, %onnx::Conv_206, %onnx::Conv_207) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_153 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_54\"](%input.75) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_208 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_55\"](%onnx::Conv_153, %onnx::Conv_209, %onnx::Conv_210) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_156 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_56\"](%onnx::Add_208, %onnx::Conv_147) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  %input.87 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_57\"](%onnx::Conv_156, %onnx::Conv_212, %onnx::Conv_213) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_159 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_58\"](%input.87) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.95 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_59\"](%onnx::Conv_159, %onnx::Conv_215, %onnx::Conv_216) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_162 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_60\"](%input.95) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_217 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_61\"](%onnx::Conv_162, %onnx::Conv_218, %onnx::Conv_219) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_165 : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_62\"](%onnx::Add_217, %onnx::Conv_156) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  %input.107 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_63\"](%onnx::Conv_165, %onnx::Conv_221, %onnx::Conv_222) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_168 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_64\"](%input.107) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %input.115 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=768, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_65\"](%onnx::Conv_168, %onnx::Conv_224, %onnx::Conv_225) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %onnx::Conv_171 : Float(*, 768, *, *, strides=[78643200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_66\"](%input.115) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/functional.py:1453:0\n",
      "  %onnx::Add_226 : Float(*, 128, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_67\"](%onnx::Conv_171, %onnx::Conv_227, %onnx::Conv_228) # /home/sajjad/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:452:0\n",
      "  %output : Float(*, *, *, *, strides=[13107200, 102400, 320, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_68\"](%onnx::Add_226, %onnx::Conv_165) # /home/sajjad/sajjad/scripts/notebook/builder.py:292:0\n",
      "  return (%output)\n",
      "\n",
      "------------- Checking exported model\n",
      "------------- Performing the profiling...\n",
      "\u001b[35m>>>>> Setting Environment Variables Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Entering convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "2022-10-19 20:49:47.715616: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:49:47.715700: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Start importing onnx...\u001b[0m\n",
      "\u001b[32mI Current ONNX Model use ir_version 3 opset_version 7\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'eliminate_option_const' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_const_branch' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'froze_if' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_sequence_construct_concat_from_sequence' success\u001b[0m\n",
      "\u001b[32mI Call acuity onnx optimize 'merge_lrn_lowlevel_implement' success\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_179 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_191 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_176 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_209 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_177 shape: [768]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_188 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_203 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_185 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_197 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Constant_onnx::Reshape_119 shape: [4]\u001b[0m\n",
      "\u001b[36mD Calc tensor Constant_onnx::Gather_230 shape: []\u001b[0m\n",
      "\u001b[36mD Calc tensor Gather_onnx::Reshape_111 shape: [1, 1024, 12800]\u001b[0m\n",
      "\u001b[36mD Calc tensor Reshape_input.1 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.7 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_123 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.15 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_126 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_215 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_212 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_182 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_206 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_183 shape: [128]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_224 shape: [768, 1, 3, 3]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_200 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_194 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_181 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_129 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.27 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_132 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.35 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_135 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_190 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_138 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.47 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_221 shape: [768, 128, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_227 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_141 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.55 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_144 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_199 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_147 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.67 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_150 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.75 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_153 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_208 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_156 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.87 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_159 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.95 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_162 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Initializer_onnx::Conv_218 shape: [128, 768, 1, 1]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_217 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_onnx::Conv_165 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.107 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_168 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_input.115 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Relu_onnx::Conv_171 shape: [1, 768, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Conv_onnx::Add_226 shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[36mD Calc tensor Add_output shape: [1, 128, 320, 320]\u001b[0m\n",
      "\u001b[32mI build output layer attach_Add_Add_68:out0\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_68:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_68']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_67:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_227', 'Conv_Conv_67', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_62:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_62']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_66:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_66']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_61:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_218', 'Conv_Conv_61', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_56:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_56']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_65:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_224', 'Conv_Conv_65', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_60:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_60']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_55:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_209', 'Conv_Conv_55', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_50:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_50']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_64:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_64']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_59:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_215', 'Conv_Conv_59', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_54:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_54']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_49:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_200', 'Conv_Conv_49', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_44:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_44']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_63:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_221', 'Conv_Conv_63', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_58:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_58']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_53:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_206', 'Conv_Conv_53', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_48:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_48']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_43:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_191', 'Conv_Conv_43', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Add_Add_38:out0\u001b[0m\n",
      "\u001b[32mI Match r_add [['Add_Add_38']] [['Add']] to [['add']]\u001b[0m\n",
      "\u001b[32mI Try match Reshape_Reshape_32:out0\u001b[0m\n",
      "\u001b[32mI Match r_rsp_v5 [['Constant_Concat_31_onnx__Reshape_119_as_const', 'Reshape_Reshape_32']] [['Reshape', 'Constant_0']] to [['reshape']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_57:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_212', 'Conv_Conv_57', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_52:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_52']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_47:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_197', 'Conv_Conv_47', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_42:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_42']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_37:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_182', 'Conv_Conv_37', 'Initializer_onnx::Conv_183']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Gather_Gather_20:out0\u001b[0m\n",
      "\u001b[32mI Match r_gather [['Gather_Gather_20']] [['Gather']] to [['gather']]\u001b[0m\n",
      "\u001b[32mI Try match Constant_Cast_19_onnx__Gather_230_as_const:out0\u001b[0m\n",
      "\u001b[32mI Match r_variable [['Constant_Cast_19_onnx__Gather_230_as_const']] [['Constant']] to [['variable']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_51:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_203', 'Conv_Conv_51', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_46:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_46']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_41:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_188', 'Conv_Conv_41', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_36:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_36']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_35:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_179', 'Conv_Conv_35', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_34:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_34']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_45:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_194', 'Conv_Conv_45', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Relu_Relu_40:out0\u001b[0m\n",
      "\u001b[32mI Match r_relu [['Relu_Relu_40']] [['Relu']] to [['relu']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_33:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_176', 'Conv_Conv_33', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI Try match Conv_Conv_39:out0\u001b[0m\n",
      "\u001b[32mI Match r_conv [['Initializer_onnx::Conv_185', 'Conv_Conv_39', 'Initializer_onnx::Conv_177']] [['Conv', 'Constant_0', 'Constant_1']] to [['convolution']]\u001b[0m\n",
      "\u001b[32mI build input layer input:out0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_67_2 0  ~ Add_Add_68_1 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_62_3 0  ~ Add_Add_68_1 1\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_66_4 0  ~ Conv_Conv_67_2 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_61_5 0  ~ Add_Add_62_3 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_56_6 0  ~ Add_Add_62_3 1\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_65_7 0  ~ Relu_Relu_66_4 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_60_8 0  ~ Conv_Conv_61_5 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_55_9 0  ~ Add_Add_56_6 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_50_10 0  ~ Add_Add_56_6 1\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_64_11 0  ~ Conv_Conv_65_7 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_59_12 0  ~ Relu_Relu_60_8 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_54_13 0  ~ Conv_Conv_55_9 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_49_14 0  ~ Add_Add_50_10 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_44_15 0  ~ Add_Add_50_10 1\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_63_16 0  ~ Relu_Relu_64_11 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_58_17 0  ~ Conv_Conv_59_12 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_53_18 0  ~ Relu_Relu_54_13 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_48_19 0  ~ Conv_Conv_49_14 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_43_20 0  ~ Add_Add_44_15 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_38_21 0  ~ Add_Add_44_15 1\u001b[0m\n",
      "\u001b[36mD connect Add_Add_62_3 0  ~ Conv_Conv_63_16 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_57_23 0  ~ Relu_Relu_58_17 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_52_24 0  ~ Conv_Conv_53_18 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_47_25 0  ~ Relu_Relu_48_19 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_42_26 0  ~ Conv_Conv_43_20 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_37_27 0  ~ Add_Add_38_21 0\u001b[0m\n",
      "\u001b[36mD connect Reshape_Reshape_32_22 0  ~ Add_Add_38_21 1\u001b[0m\n",
      "\u001b[36mD connect Gather_Gather_20_28 0  ~ Reshape_Reshape_32_22 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_56_6 0  ~ Conv_Conv_57_23 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_51_30 0  ~ Relu_Relu_52_24 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_46_31 0  ~ Conv_Conv_47_25 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_41_32 0  ~ Relu_Relu_42_26 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_36_33 0  ~ Conv_Conv_37_27 0\u001b[0m\n",
      "\u001b[36mD connect input_40 0  ~ Gather_Gather_20_28 0\u001b[0m\n",
      "\u001b[36mD connect Constant_Cast_19_onnx__Gather_230_as_const_29 0  ~ Gather_Gather_20_28 1\u001b[0m\n",
      "\u001b[36mD connect Add_Add_50_10 0  ~ Conv_Conv_51_30 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_45_36 0  ~ Relu_Relu_46_31 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_40_37 0  ~ Conv_Conv_41_32 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_35_34 0  ~ Relu_Relu_36_33 0\u001b[0m\n",
      "\u001b[36mD connect Relu_Relu_34_35 0  ~ Conv_Conv_35_34 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_33_38 0  ~ Relu_Relu_34_35 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_44_15 0  ~ Conv_Conv_45_36 0\u001b[0m\n",
      "\u001b[36mD connect Conv_Conv_39_39 0  ~ Relu_Relu_40_37 0\u001b[0m\n",
      "\u001b[36mD connect Reshape_Reshape_32_22 0  ~ Conv_Conv_33_38 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_38_21 0  ~ Conv_Conv_39_39 0\u001b[0m\n",
      "\u001b[36mD connect Add_Add_68_1 0  ~ attach_Add_Add_68/out0_0 0\u001b[0m\n",
      "2022-10-19 20:49:53.186245: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 20:49:53.373700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793065000 Hz\n",
      "2022-10-19 20:49:53.374274: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x686a430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-19 20:49:53.374329: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-19 20:49:53.392990: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:49:53.393018: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-19 20:49:53.393041: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d719d7d4fe05): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36mD Process input_40 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_40:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_32_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_32_22:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_34_35 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_34_35:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_35_34:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_36_33 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_36_33:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_37_27:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_40_37 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_40_37:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_41_32:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_42_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_42_26:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_43_20:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_46_31 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_46_31:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_47_25:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_48_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_48_19:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_49_14:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_52_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_52_24:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_53_18:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_54_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_54_13:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_55_9:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_58_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_58_17:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_59_12:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_60_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_60_8:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_61_5:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_64_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_64_11:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_65_7:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_66_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_66_4:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_67_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_68/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_68/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Start C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Optimizing network with broadcast_op\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_68_1_acuity_mark_perm_41 before Add_Add_68_1\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_63_16_acuity_mark_perm_42 before Conv_Conv_63_16\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_62_3_acuity_mark_perm_43 before Add_Add_62_3\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_57_23_acuity_mark_perm_44 before Conv_Conv_57_23\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_56_6_acuity_mark_perm_45 before Add_Add_56_6\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_51_30_acuity_mark_perm_46 before Conv_Conv_51_30\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_50_10_acuity_mark_perm_47 before Add_Add_50_10\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_45_36_acuity_mark_perm_48 before Conv_Conv_45_36\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_44_15_acuity_mark_perm_49 before Add_Add_44_15\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_39_39_acuity_mark_perm_50 before Conv_Conv_39_39\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_33_38_acuity_mark_perm_51 before Conv_Conv_33_38\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_38_21_acuity_mark_perm_52 before Add_Add_38_21\u001b[0m\n",
      "\u001b[32mI End C2T Switcher...\u001b[0m\n",
      "\u001b[36mD Process input_40 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_40:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_32_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_32_22:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38_acuity_mark_perm_51 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38_acuity_mark_perm_51:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_34_35 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_34_35:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_35_34:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_36_33 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_36_33:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_37_27:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21_acuity_mark_perm_52 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21_acuity_mark_perm_52:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39_acuity_mark_perm_50 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39_acuity_mark_perm_50:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_40_37 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_40_37:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_41_32:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_42_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_42_26:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_43_20:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15_acuity_mark_perm_49 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15_acuity_mark_perm_49:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36_acuity_mark_perm_48 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36_acuity_mark_perm_48:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_46_31 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_46_31:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_47_25:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_48_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_48_19:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_49_14:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10_acuity_mark_perm_47 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10_acuity_mark_perm_47:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30_acuity_mark_perm_46 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30_acuity_mark_perm_46:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_52_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_52_24:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_53_18:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_54_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_54_13:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_55_9:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6_acuity_mark_perm_45 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6_acuity_mark_perm_45:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23_acuity_mark_perm_44 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23_acuity_mark_perm_44:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_58_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_58_17:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_59_12:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_60_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_60_8:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_61_5:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3_acuity_mark_perm_43 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3_acuity_mark_perm_43:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16_acuity_mark_perm_42 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16_acuity_mark_perm_42:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_64_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_64_11:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_65_7:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_66_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_66_4:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_67_2:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1_acuity_mark_perm_41 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1_acuity_mark_perm_41:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1:out0 type: float32\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_68/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_68/out0_0:out0 type: float32\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[36mD Optimizing network with force_1d_tensor, swapper, merge_duplicate_quantize_dequantize, merge_layer, auto_fill_bn, auto_fill_l2normalizescale, auto_fill_instancenormalize, resize_nearest_transformer, auto_fill_multiply, compute_gather_negative, auto_fill_zero_bias, proposal_opt_import, special_add_to_conv2d, extend_gather_to_gather_reshape\u001b[0m\n",
      "\u001b[32mI End importing onnx...\u001b[0m\n",
      "\u001b[32mI Dump net to mnist.json\u001b[0m\n",
      "\u001b[32mI Save net to mnist.data\u001b[0m\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "-------------------- QUANTIZATION SCRIPT\n",
      "2022-10-19 20:49:55.092780: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:49:55.092813: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Namespace(action='quantization', batch_size=1, caffe_mean_file=None, capture_format='nchw', capture_quantized=False, channel_mean_value='127 127 127 255', config=None, data_output=None, debug=True, device=None, divergence_first_quantize_bits=11, dtype='float', epochs=1, epochs_per_decay=100, force_gray=False, fpfs_delta0=1, fpfs_epochs=0, fpfs_reduce_target=0, input_fitting='scale', input_normalization=None, lr=0.1, mean_file=None, model_data='mnist.data', model_data_format='zone', model_input='mnist.json', model_quantize=None, optimizer='momentum', output_dir=None, output_num=5, pb_name=None, pfps_delta0=1, pfps_epochs=0, pfps_reduce_target=0, prune_epochs=10, prune_loss=1, quantized_algorithm='normal', quantized_divergence_nbins=0, quantized_dtype='asymmetric_affine-u8', quantized_hybrid=False, quantized_moving_alpha=0.0, quantized_rebuild=False, quantized_rebuild_all=True, random_brightness=None, random_contrast=None, random_crop=False, random_flip=False, random_mirror=False, reorder_channel=None, restart=False, samples=-1, source='text', source_file='dataset/dataset1.txt', task='classification', validation_output='validation.csv', without_update_masked_grad=False)\u001b[0m\n",
      "2022-10-19 20:49:56.452879: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 20:49:56.477690: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793065000 Hz\n",
      "2022-10-19 20:49:56.478209: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63db970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-19 20:49:56.478242: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-19 20:49:56.479349: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:49:56.479362: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-19 20:49:56.479374: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d719d7d4fe05): /proc/driver/nvidia/version does not exist\n",
      "\u001b[32mI Load model in mnist.json\u001b[0m\n",
      "\u001b[32mI Load data in mnist.data\u001b[0m\n",
      "W:tensorflow:From acuitylib/app/tensorzone/workspace.py:26: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "\u001b[32mI Fitting image with scale.\u001b[0m\n",
      "\u001b[32mI Channel mean value [127.0, 127.0, 127.0, 255.0]\u001b[0m\n",
      "\u001b[32mI [TRAINER]Quantization start...\u001b[0m\n",
      "[TRAINER]Quantization start...\n",
      "\u001b[32mI Init validate tensor provider.\u001b[0m\n",
      "\u001b[32mI Enqueue samples 20\u001b[0m\n",
      "\u001b[32mI Init provider with 20 samples.\u001b[0m\n",
      "\u001b[36mD set up a quantize net\u001b[0m\n",
      "\u001b[36mD Process input_40 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_40:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 3, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Real output shape: (1,)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 1, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28_acuity_opt_gather_reshape_53 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28_acuity_opt_gather_reshape_53:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 1024, 12800)\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_32_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_32_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38_acuity_mark_perm_51 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38_acuity_mark_perm_51:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_34_35 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_34_35:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_35_34:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_36_33 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_36_33:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_37_27:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21_acuity_mark_perm_52 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21_acuity_mark_perm_52:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39_acuity_mark_perm_50 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39_acuity_mark_perm_50:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_40_37 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_40_37:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_41_32:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_42_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_42_26:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_43_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15_acuity_mark_perm_49 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15_acuity_mark_perm_49:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36_acuity_mark_perm_48 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36_acuity_mark_perm_48:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_46_31 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_46_31:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_47_25:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_48_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_48_19:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_49_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10_acuity_mark_perm_47 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10_acuity_mark_perm_47:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30_acuity_mark_perm_46 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30_acuity_mark_perm_46:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_52_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_52_24:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_53_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_54_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_54_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_55_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6_acuity_mark_perm_45 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6_acuity_mark_perm_45:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23_acuity_mark_perm_44 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23_acuity_mark_perm_44:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_58_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_58_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_59_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_60_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_60_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_61_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3_acuity_mark_perm_43 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3_acuity_mark_perm_43:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16_acuity_mark_perm_42 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16_acuity_mark_perm_42:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_64_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_64_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_65_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_66_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_66_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 768)\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_67_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 320, 320, 128)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1_acuity_mark_perm_41 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1_acuity_mark_perm_41:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_68/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_68/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Real output shape: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Generated network graph with 1 outputs.\u001b[0m\n",
      "\u001b[32mI  @attach_Add_Add_68/out0_0:out0: (1, 128, 320, 320)\u001b[0m\n",
      "\u001b[36mD Init coefficients ...\u001b[0m\n",
      "\u001b[32mI Start tensor porvider ... \u001b[0m\n",
      "\u001b[32mI Runing 1 epochs, algorithm: normal\u001b[0m\n",
      "\u001b[32mI iterations: 0\u001b[0m\n",
      "\u001b[36mD Quantize tensor @attach_Add_Add_68/out0_0:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_68_1:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_67_2:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_62_3:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_66_4:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_61_5:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_56_6:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_65_7:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_60_8:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_55_9:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_50_10:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_64_11:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_59_12:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_54_13:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_49_14:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_44_15:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_63_16:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_58_17:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_53_18:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_48_19:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_43_20:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_38_21:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Reshape_Reshape_32_22:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_57_23:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_52_24:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_47_25:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_42_26:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_37_27:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Gather_Gather_20_28:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Constant_Cast_19_onnx__Gather_230_as_const_29:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_51_30:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_46_31:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_41_32:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_36_33:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_35_34:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_34_35:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_45_36:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Relu_Relu_40_37:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_33_38:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_39_39:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @input_40:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_68_1_acuity_mark_perm_41:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_63_16_acuity_mark_perm_42:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_62_3_acuity_mark_perm_43:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_57_23_acuity_mark_perm_44:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_56_6_acuity_mark_perm_45:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_51_30_acuity_mark_perm_46:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_50_10_acuity_mark_perm_47:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_45_36_acuity_mark_perm_48:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_44_15_acuity_mark_perm_49:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_39_39_acuity_mark_perm_50:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_33_38_acuity_mark_perm_51:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Add_Add_38_21_acuity_mark_perm_52:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Gather_Gather_20_28_acuity_opt_gather_reshape_53:out0.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_67_2:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_61_5:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_65_7:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_55_9:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_59_12:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_49_14:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_63_16:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_53_18:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_43_20:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_57_23:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_47_25:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_37_27:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_51_30:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_41_32:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_35_34:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_45_36:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_33_38:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_39_39:weight.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_67_2:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_61_5:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_65_7:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_55_9:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_59_12:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_49_14:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_63_16:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_53_18:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_43_20:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_57_23:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_47_25:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_37_27:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_51_30:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_41_32:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_35_34:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_45_36:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_33_38:bias.\u001b[0m\n",
      "\u001b[36mD Quantize tensor @Conv_Conv_39_39:bias.\u001b[0m\n",
      "\u001b[32mI Clean.\u001b[0m\n",
      "\u001b[36mD Optimizing network with align_quantize, broadcast_quantize, qnt_adjust_coef, qnt_adjust_param\u001b[0m\n",
      "\u001b[32mI Dump net quantize tensor table to mnist.quantize\u001b[0m\n",
      "\u001b[32mI [TRAINER]Quantization complete.\u001b[0m\n",
      "[TRAINER]Quantization complete.\n",
      "\u001b[32mI Save net to mnist.data\u001b[0m\n",
      "\u001b[32mI Clean.\u001b[0m\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "-------------------- EXPORT CASE\n",
      "2022-10-19 20:50:09.121995: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:50:09.122028: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[32mI Load model in mnist.json\u001b[0m\n",
      "\u001b[32mI Load data in mnist.data\u001b[0m\n",
      "\u001b[32mI Load quantization tensor table mnist.quantize\u001b[0m\n",
      "2022-10-19 20:50:11.023480: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 20:50:11.045691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3793065000 Hz\n",
      "2022-10-19 20:50:11.046228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59234b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-19 20:50:11.046255: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-19 20:50:11.047317: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /acuity-toolkit/bin/acuitylib\n",
      "2022-10-19 20:50:11.047333: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-19 20:50:11.047348: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d719d7d4fe05): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36mD Process input_40 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_40:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28_acuity_opt_gather_reshape_53 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28_acuity_opt_gather_reshape_53:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_32_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_32_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38_acuity_mark_perm_51 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38_acuity_mark_perm_51:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_34_35 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_34_35:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_35_34:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_36_33 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_36_33:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_37_27:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21_acuity_mark_perm_52 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21_acuity_mark_perm_52:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39_acuity_mark_perm_50 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39_acuity_mark_perm_50:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_40_37 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_40_37:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_41_32:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_42_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_42_26:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_43_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15_acuity_mark_perm_49 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15_acuity_mark_perm_49:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36_acuity_mark_perm_48 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36_acuity_mark_perm_48:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_46_31 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_46_31:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_47_25:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_48_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_48_19:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_49_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10_acuity_mark_perm_47 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10_acuity_mark_perm_47:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30_acuity_mark_perm_46 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30_acuity_mark_perm_46:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_52_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_52_24:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_53_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_54_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_54_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_55_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6_acuity_mark_perm_45 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6_acuity_mark_perm_45:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23_acuity_mark_perm_44 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23_acuity_mark_perm_44:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_58_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_58_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_59_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_60_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_60_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_61_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3_acuity_mark_perm_43 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3_acuity_mark_perm_43:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16_acuity_mark_perm_42 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16_acuity_mark_perm_42:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_64_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_64_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_65_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_66_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 320 320 768)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_66_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 320 320 128)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_67_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1_acuity_mark_perm_41 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(permute): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1_acuity_mark_perm_41:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_68/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_68/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[32mI Initialzing network optimizer by /acuity-toolkit/convertdemo/../bin/VIPNANOQI_PID0X88 ...\u001b[0m\n",
      "\u001b[36mD Optimizing network with merge_ximum, qnt_adjust_coef, multiply_transform, add_extra_io, format_input_ops, auto_fill_zero_bias, conv_kernel_transform, strip_op, extend_unstack_split, merge_layer, transform_layer, broadcast_op, strip_op, auto_fill_reshape_zero, adjust_output_attrs, insert_dtype_converter\u001b[0m\n",
      "\u001b[36mD Strip layer Gather_Gather_20_28_acuity_opt_gather_reshape_53(reshape)\u001b[0m\n",
      "\u001b[36mD Insert dtype_converter Constant_Cast_19_onnx__Gather_230_as_const_29_dtype_convert_Gather_Gather_20_28 between Constant_Cast_19_onnx__Gather_230_as_const_29 and Gather_Gather_20_28\u001b[0m\n",
      "\u001b[32mI Start T2C Switcher...\u001b[0m\n",
      "\u001b[36mD Optimizing network with broadcast_op, t2c_fc\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_68_1_acuity_mark_perm_41_acuity_mark_perm_54 before Add_Add_68_1_acuity_mark_perm_41\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_62_3_acuity_mark_perm_43_acuity_mark_perm_55 before Add_Add_62_3_acuity_mark_perm_43\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_56_6_acuity_mark_perm_45_acuity_mark_perm_56 before Add_Add_56_6_acuity_mark_perm_45\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_50_10_acuity_mark_perm_47_acuity_mark_perm_57 before Add_Add_50_10_acuity_mark_perm_47\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_44_15_acuity_mark_perm_49_acuity_mark_perm_58 before Add_Add_44_15_acuity_mark_perm_49\u001b[0m\n",
      "\u001b[36mD insert permute Add_Add_38_21_acuity_mark_perm_52_acuity_mark_perm_59 before Add_Add_38_21_acuity_mark_perm_52\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_63_16_acuity_mark_perm_60 before Conv_Conv_63_16\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_57_23_acuity_mark_perm_61 before Conv_Conv_57_23\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_51_30_acuity_mark_perm_62 before Conv_Conv_51_30\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_45_36_acuity_mark_perm_63 before Conv_Conv_45_36\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_39_39_acuity_mark_perm_64 before Conv_Conv_39_39\u001b[0m\n",
      "\u001b[36mD insert permute Conv_Conv_33_38_acuity_mark_perm_65 before Conv_Conv_33_38\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_33_38_acuity_mark_perm_51\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_33_38_acuity_mark_perm_65\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_38_21_acuity_mark_perm_52_acuity_mark_perm_59\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_38_21_acuity_mark_perm_52\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_39_39_acuity_mark_perm_50\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_39_39_acuity_mark_perm_64\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_44_15_acuity_mark_perm_49_acuity_mark_perm_58\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_44_15_acuity_mark_perm_49\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_45_36_acuity_mark_perm_48\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_45_36_acuity_mark_perm_63\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_50_10_acuity_mark_perm_47_acuity_mark_perm_57\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_50_10_acuity_mark_perm_47\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_51_30_acuity_mark_perm_46\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_51_30_acuity_mark_perm_62\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_56_6_acuity_mark_perm_45_acuity_mark_perm_56\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_56_6_acuity_mark_perm_45\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_57_23_acuity_mark_perm_44\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_57_23_acuity_mark_perm_61\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_62_3_acuity_mark_perm_43_acuity_mark_perm_55\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_62_3_acuity_mark_perm_43\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_63_16_acuity_mark_perm_42\u001b[0m\n",
      "\u001b[36mD remove permute Conv_Conv_63_16_acuity_mark_perm_60\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_68_1_acuity_mark_perm_41_acuity_mark_perm_54\u001b[0m\n",
      "\u001b[36mD remove permute Add_Add_68_1_acuity_mark_perm_41\u001b[0m\n",
      "\u001b[32mI End T2C Switcher...\u001b[0m\n",
      "\u001b[36mD Process input_40 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(input): (1 3 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @input_40:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(variable): (1)\u001b[0m\n",
      "\u001b[36mD Process Constant_Cast_19_onnx__Gather_230_as_const_29_dtype_convert_Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(dtype_converter): (1)\u001b[0m\n",
      "\u001b[36mD Tensor @Constant_Cast_19_onnx__Gather_230_as_const_29_dtype_convert_Gather_Gather_20_28:out0 type: int64\u001b[0m\n",
      "\u001b[36mD Process Gather_Gather_20_28 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(gather): (1 1 1024 12800)\u001b[0m\n",
      "\u001b[36mD Tensor @Gather_Gather_20_28:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Reshape_Reshape_32_22 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(reshape): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Reshape_Reshape_32_22:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_33_38:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_34_35 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_34_35:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_35_34:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_36_33 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_36_33:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_37_27:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_38_21 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_38_21:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_39_39:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_40_37 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_40_37:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_41_32:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_42_26 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_42_26:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_43_20:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_44_15 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_44_15:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_45_36:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_46_31 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_46_31:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_47_25:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_48_19 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_48_19:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_49_14:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_50_10 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_50_10:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_51_30:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_52_24 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_52_24:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_53_18:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_54_13 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_54_13:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_55_9:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_56_6 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_56_6:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_57_23:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_58_17 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_58_17:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_59_12:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_60_8 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_60_8:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_61_5:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_62_3 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_62_3:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_63_16:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_64_11 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_64_11:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_65_7:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Relu_Relu_66_4 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(relu): (1 768 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Relu_Relu_66_4:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(convolution): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Conv_Conv_67_2:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process Add_Add_68_1 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(add): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @Add_Add_68_1:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[36mD Process attach_Add_Add_68/out0_0 ...\u001b[0m\n",
      "\u001b[36mD Acuity output shape(output): (1 128 320 320)\u001b[0m\n",
      "\u001b[36mD Tensor @attach_Add_Add_68/out0_0:out0 type: asymmetric_affine\u001b[0m\n",
      "\u001b[32mI Build torch_jit complete.\u001b[0m\n",
      "\u001b[36mD Optimizing network with conv_1xn_transform, proposal_opt, c2drv_convert_axis, c2drv_convert_shape, c2drv_convert_array, c2drv_cast_dtype, c2drv_trans_data\u001b[0m\n",
      "\u001b[32mI Building data ...\u001b[0m\n",
      "\u001b[32mI Packing data ...\u001b[0m\n",
      "\u001b[36mD Packing Constant_Cast_19_onnx__Gather_230_as_const_29 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Constant_Cast_19_onnx__Gather_230_as_const_29:data to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_33_38 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_33_38:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_33_38:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_35_34 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_35_34:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_35_34:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_37_27 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_37_27:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_37_27:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_39_39 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_39_39:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_39_39:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_41_32 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_41_32:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_41_32:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_43_20 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_43_20:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_43_20:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_45_36 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_45_36:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_45_36:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_47_25 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_47_25:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_47_25:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_49_14 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_49_14:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_49_14:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_51_30 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_51_30:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_51_30:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_53_18 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_53_18:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_53_18:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_55_9 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_55_9:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_55_9:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_57_23 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_57_23:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_57_23:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_59_12 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_59_12:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_59_12:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_61_5 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_61_5:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_61_5:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_63_16 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_63_16:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_63_16:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_65_7 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_65_7:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_65_7:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Packing Conv_Conv_67_2 ...\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_67_2:bias to asymmetric_affine.\u001b[0m\n",
      "\u001b[36mD Quantize @Conv_Conv_67_2:weight to asymmetric_affine.\u001b[0m\n",
      "\u001b[32mI Saving data to mnist.export.data\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_mnist.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_mnist.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_post_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_post_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_pre_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_pre_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/vnn_global.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/main.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/BUILD\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/mnist.vcxproj\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/makefile.linux\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/.cproject\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo/.project\u001b[0m\n",
      "\u001b[36mD Generate fake input /acuity-toolkit/convertdemo/input_40_0.tensor\u001b[0m\n",
      "mv: '/acuity-toolkit/convertdemo/network_binary.nb' and '/acuity-toolkit/convertdemo/network_binary.nb' are the same file\n",
      "mv: '/acuity-toolkit/convertdemo/input_0.dat' and '/acuity-toolkit/convertdemo/input_0.dat' are the same file\n",
      "mv: '/acuity-toolkit/convertdemo/output0_320_320_128_1.dat' and '/acuity-toolkit/convertdemo/output0_320_320_128_1.dat' are the same file\n",
      "\u001b[32mI Dump nbg input meta to /acuity-toolkit/convertdemo_nbg_unify/nbg_meta.json\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_mnist.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_mnist.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_post_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_post_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_pre_process.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_pre_process.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/vnn_global.h\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/main.c\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/BUILD\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/mnist.vcxproj\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/makefile.linux\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/.cproject\u001b[0m\n",
      "\u001b[32mI Save vx network source file to /acuity-toolkit/convertdemo_nbg_unify/.project\u001b[0m\n",
      "/acuity-toolkit/convertdemo_nbg_unify\n",
      "customer:input,0,1:output,0,0:\n",
      "*********************************\n",
      "/acuity-toolkit/convertdemo\n",
      "/\n",
      "/acuity-toolkit/convertdemo_nbg_unify\n",
      "\u001b[32mI ----------------Error(0),Warning(0)----------------\u001b[0m\n",
      "\u001b[35m>>>>> Exiting convert-mnist-onnx-to-khadas.sh ... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "patching file main.c\n",
      "\u001b[35m>>>>> Patching main.c on Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_pre_process.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_mnist.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vnn_mnist.c: In function ‘vnn_CreateMnist’:\n",
      "vnn_mnist.c:146:29: warning: unused variable ‘data’ [-Wunused-variable]\n",
      "  146 |     uint8_t *               data;\n",
      "      |                             ^~~~\n",
      "At top level:\n",
      "vnn_mnist.c:94:17: warning: ‘load_data’ defined but not used [-Wunused-function]\n",
      "   94 | static uint8_t* load_data\n",
      "      |                 ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COMPILE /home/khadas/nbg_unify_mnist/main.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_post_process.c\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "------------- Performing the profiling done!\n",
      "------------- Parsing the profiling results...\n",
      "\u001b[35m>>>>> Executing network on Khadas is saved in model_execution.log <<<<<<<\u001b[37m\n",
      "Parsing ../logs/model_execution.log....\n",
      "------------- Parsing the profiling results done!\n",
      "------------- auto_profile done!...\n"
     ]
    }
   ],
   "source": [
    "loop_run = '10'\n",
    "MP = prof.ModelProfile()\n",
    "[ProfileArray,status] = MP.doProfiling(model,\n",
    "                 loop_run,\n",
    "                 gen_img_input_channels,\n",
    "                 gen_img_input_dim_h,\n",
    "                 gen_img_input_dim_w,\n",
    "                 debug=True)\n",
    "\n",
    "accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62bb490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "['Create Neural Network' 'Verify Graph' 'Run the 1 time' 'Run the 2 time'\n",
      " 'Run the 3 time' 'Run the 4 time' 'Run the 5 time' 'Run the 6 time'\n",
      " 'Run the 7 time' 'Run the 8 time' 'Run the 9 time' 'Run the 10 time'\n",
      " 'Total   ' 'Average ']\n",
      "---------\n",
      "\n",
      "[' 91492us' ' 50566us' ' 1542983.00us' ' 1538697.00us' ' 1539738.00us'\n",
      " ' 1540626.00us' ' 1540729.00us' ' 1537278.00us' ' 1541310.00us'\n",
      " ' 1539103.00us' ' 1541940.00us' ' 1539051.00us' ' 15401659.00us'\n",
      " ' 1540165.88us']\n",
      "---------\n",
      "\n",
      "['91492' '50566' '1542983' '1538697' '1539738' '1540626' '1540729'\n",
      " '1537278' '1541310' '1539103' '1541940' '1539051' '15401659' '1540165']\n",
      "---------\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "C0 = np.array(ProfileArray[0])\n",
    "C1 = np.array(ProfileArray[1])\n",
    "C2 = np.array(ProfileArray[2])\n",
    "print('---------\\n')\n",
    "print(C0)\n",
    "print('---------\\n')\n",
    "print(C1)\n",
    "print('---------\\n')\n",
    "print(C2)\n",
    "print('---------\\n')\n",
    "print(status)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
