{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f945bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from nni.algorithms.compression.pytorch.quantization import LsqQuantizer, QAT_Quantizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.numpy_helper\n",
    "from math import ceil\n",
    "### markus\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required_input_dim = 3121\n",
    "#gen_img_input_dim_w = required_input_dim\n",
    "#gen_img_input_dim_h = 1\n",
    "#gen_img_input_channels = 3\n",
    "\n",
    "#imgSizeW=32\n",
    "imgSizeW= 82#ceil(imgSizeW/4)*4\n",
    "#imgSizeH=28\n",
    "imgSizeH= 1#ceil(imgSizeH/4)*4\n",
    "\n",
    "gen_img_input_dim_w = imgSizeW\n",
    "gen_img_input_dim_h = imgSizeH\n",
    "gen_img_input_channels = 3\n",
    "#test input image\n",
    "test_input_data=\"../convertdemo/dataset/rand_3.jpg\"\n",
    "#Paths\n",
    "quant_image_path = \"../quantization_images\"\n",
    "script_path = \"../scripts\"\n",
    "log_path = \"../logs\"\n",
    "network_path = \"../convertdemo/network\"\n",
    "#Files\n",
    "perform_script = \"perform_r6.sh\"\n",
    "parse_script = \"parse_r1.sh\"\n",
    "perform_log_file = \"model_execution.log\"\n",
    "parsed_log_file = \"model_execution_parsed.log\"\n",
    "model_name=\"mnist\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3c22",
   "metadata": {},
   "source": [
    "# Generate images based on some arbitrary input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4177221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = quant_image_path\n",
    "\n",
    "def generate_random_images(xdim, ydim, channels=3, count=1, path=\".\"):\n",
    "    \"\"\"\n",
    "    This functions generates random bmp images to use for quantization given\n",
    "    a defined dimension\n",
    "        @xdim   .. width of images\n",
    "        @ydim   .. height of images\n",
    "        @count  .. number of images\n",
    "        @path   .. path of images\n",
    "    \"\"\"\n",
    "\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(path)\n",
    "        print(\"The new directory quantization_images is created!\")\n",
    "\n",
    "    #delete the pre generated bmp/jpg files\n",
    "    filelist = [ f for f in os.listdir(quant_image_path) if (f.endswith(\".jpg\") or f.endswith(\".bmp\") ) ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(quant_image_path, f))\n",
    "        \n",
    "    for c in range(count):\n",
    "        rnd_img = np.random.randint(low=0,high=255, size=(ydim, xdim, channels),dtype=np.uint8) #imag.transpose((1,2,0)\n",
    "        imag_tp = np.ascontiguousarray(rnd_img, dtype=np.uint8)\n",
    "\n",
    "        pil_image = PIL.Image.frombytes('RGB',(xdim, ydim), imag_tp)\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".bmp\")\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b28be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "# choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## generate 10 random input images based on the provided dimensions\n",
    "generate_random_images(gen_img_input_dim_w, gen_img_input_dim_h, channels=3, count=20, path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71df62",
   "metadata": {},
   "source": [
    "# Build the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7aa64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(gen_img_input_dim_w, 50)#gen_img_input_dim_w*gen_img_input_dim_h\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # select first dim from [1,C,H,W] ie. [0,:,:,:]\n",
    "        x = x.select(0,0)\n",
    "        # select first channel [0,:,:]\n",
    "        x = x.select(0,0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368a16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d67f1",
   "metadata": {},
   "source": [
    "# Train the model for random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6de45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_images(count=1, path=\".\", extension='*.bmp'):\n",
    "    \"\"\"\n",
    "    loads images as np arrays; no normalization\n",
    "    \"\"\"\n",
    "    imgs =  []\n",
    "    files = glob.glob(path + \"/\" + extension)\n",
    "    for i, f in enumerate(files):\n",
    "        img = np.asarray(PIL.Image.open(f), dtype=np.float32).transpose((2,0,1))\n",
    "        img = img.reshape( (1,img.shape[0],img.shape[1], img.shape[2]) )\n",
    "        img = torch.from_numpy(img)\n",
    "        imgs.append(img)\n",
    "\n",
    "        if i+1 > count:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = load_images(count=20,path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40f3c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.232093811035156\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for img in imgs:\n",
    "    target = torch.tensor([2])\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    #print(output)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    print(loss.item())\n",
    "    #print(target, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f6abb",
   "metadata": {},
   "source": [
    "# Now Assume the Model is Given\n",
    "- Use it to Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2908e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_to_onnx(model, input_shape=(1,3,28,28), path=model_name+\".onnx\"):\n",
    "\n",
    "\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    model.to('cpu')\n",
    "        \n",
    "    # very important or must leave out - not sure need to test again...\n",
    "    #traced = torch.jit.trace(model, input_dimension)\n",
    "    print(\"------------- Exporting to onnx\")\n",
    "    torch.onnx.export(\n",
    "                      model, \n",
    "                      dummy_input, \n",
    "                      path,\n",
    "                      opset_version=7,\n",
    "                      verbose=True,\n",
    "                      export_params=True, \n",
    "                      input_names=['input'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes=None\n",
    "    )\n",
    "    \n",
    "    print(\"------------- Checking exported model\")\n",
    "    \n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(path)\n",
    "\n",
    "    # Check that the IR is well formed\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    # Print a Human readable representation of the graph\n",
    "    print( onnx.helper.printable_graph(onnx_model.graph) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3cfa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_model_to_onnx(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d1c11",
   "metadata": {},
   "source": [
    "#Create a image dimensions configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e3379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(network_path+\"/\"+\"imgSize.config\", 'w') as f:\n",
    "    f.write('imgSize=%d,%d,%d'%(gen_img_input_channels,gen_img_input_dim_h,gen_img_input_dim_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788d7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import L\n",
    "import subprocess\n",
    "from subprocess import DEVNULL, STDOUT\n",
    "from xmlrpc.client import boolean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def parse_the_results(inp=\"model_execution.log\",\n",
    "                      out=\"model_execution_parsed.log\",\n",
    "                      script=\"parse.sh\",\n",
    "                      loop=1,\n",
    "                      show=False):\n",
    "    \"\"\"\n",
    "    parse the output of the profiled log\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp : input file\n",
    "        file to be parsed\n",
    "    out : output file\n",
    "        the output parsed file\n",
    "    scripts : parsing scripts\n",
    "        shell scripts to be used for parsing\n",
    "    loop: number of runs\n",
    "        the loops to run the model in the main.c\n",
    "    show: show the scripts output\n",
    "        to sohw or hide the shell script output\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"------------- Parsing the profiling results...\")\n",
    "    if show == False:\n",
    "        #subprocess.check_call([script, inp, out,loop], stdout=DEVNULL, stderr=STDOUT)\n",
    "        !bash {script} {inp}  {out} {loop}\n",
    "        \n",
    "    else:\n",
    "        #subprocess.check_call([script, inp, out,loop])\n",
    "        !bash {script} {inp}  {out} {loop} > ../logs/jupyter_parse.log\n",
    "    \n",
    "    print(\"------------- Parsing the profiling results done!\")\n",
    "\n",
    "def run_profiler(script=script_path+\"/\"+\"perform.sh\",loop=1,test_input=\"test.jpg\",lgofile=\"model_execution_parsed.log\",show=False):\n",
    "    \"\"\"\n",
    "    profile the model\n",
    "    Parameters\n",
    "    ----------\n",
    "    scripts : parsing scripts\n",
    "        shell scripts to be used for parsing\n",
    "    loop: number of runs\n",
    "        the loops to run the model in the main.c\n",
    "    test_input: input image with path\n",
    "        input image of said dimensions\n",
    "    lgofile: log file for profiling\n",
    "        output log file for profiling\n",
    "    show: show the scripts output\n",
    "        to sohw or hide the shell script output\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"------------- Performing the profiling...\")\n",
    "    if show == False:\n",
    "        #subprocess.check_call([script, loop, test_input,lgofile], stdout=DEVNULL, stderr=STDOUT)\n",
    "        !bash {script} {loop}  {test_input} {lgofile}\n",
    "    else:\n",
    "        #subprocess.check_call([script, loop, test_input,lgofile])\n",
    "        !bash {script} {loop}  {test_input} {lgofile} > ../logs/jupyter.log\n",
    "    \n",
    "    print(\"------------- Performing the profiling done!\")\n",
    "\n",
    "\n",
    "\n",
    "def auto_profile(model,\n",
    "                 loop=1,\n",
    "                 imgChannel=3,\n",
    "                 imgDimX=28,\n",
    "                 imgDimY=28,\n",
    "                 modelwithPath=model_name+\".onnx\",\n",
    "                 testingInput=\"../convertdemo/dataset/mnist2.jpg\",\n",
    "                 performScript = script_path+\"/\"+\"perform.sh\",\n",
    "                 parseScript = script_path+\"/\"+\"parse.sh\",\n",
    "                 performLogFile = log_path+\"/\"+\"model_execution.log\",\n",
    "                 parsedLogFile = log_path+\"/\"+\"model_execution_parsed.log\",\n",
    "                 debug=False):\n",
    "    \"\"\"\n",
    "    Convert torch model to onnx model and get layer bits config of onnx model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch model\n",
    "        The model to speedup by quantization\n",
    "    loop : loop\n",
    "        the number of loops to run the model on khadas in main.c\n",
    "    imgChannel: input channels\n",
    "        input image channel\n",
    "    imgDimX: input width\n",
    "        image width\n",
    "    imgDimY: input height\n",
    "        image height\n",
    "    modelwithPath: absolution model path\n",
    "        the onnx model with path\n",
    "    testingInput: input image with path\n",
    "        input image of said dimensions\n",
    "    performScript: profiling script\n",
    "        profiling script that implements whole flow\n",
    "    parseScript: parse script\n",
    "        parse script which parses the profiling log\n",
    "    performLogFile: log file for profiling\n",
    "        output log file for profiling\n",
    "    parsedLogFile: log file for parsed profiling\n",
    "        output log file for parsed profiling\n",
    "    debug : show debugging\n",
    "        show the debugging output of the scripts\n",
    "    Returns\n",
    "    -------\n",
    "    pandas frame\n",
    "        contains the execution times (profiled time)\n",
    "    status\n",
    "        the error flag indicating the status\n",
    "    \"\"\"\n",
    "    profilingDone = False;\n",
    "    #export_model_to_onnx(model,input_shape=(1,imgChannel,imgDimX,imgDimY), path=modelwithPath)\n",
    "    export_model_to_onnx(model,input_shape=(1,imgChannel,gen_img_input_dim_h,gen_img_input_dim_w), path=modelwithPath)\n",
    "\n",
    "    #sajjad@teco:~/sajjad/scripts/notebook$ ../scripts/perform_r6.sh 10 ../convertdemo/dataset/mnist2.jpg ../logs/model_execution.log\n",
    "    run_profiler(performScript,loop,testingInput,performLogFile,debug)\n",
    "\n",
    "    parse_the_results(performLogFile,parsedLogFile,parseScript,loop,debug)\n",
    "\n",
    "    #read the results into the pandas\n",
    "    profiledFrames=pd.read_csv(parsedLogFile, sep=':',header = None)\n",
    "\n",
    "    profilingDone = True;\n",
    "    return profiledFrames,profilingDone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133ee91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Exporting to onnx\n",
      "Exported graph: graph(%input : Float(1, 3, 1, 82, strides=[246, 82, 82, 1], requires_grad=0, device=cpu),\n",
      "      %fc1.weight : Float(50, 82, strides=[82, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(50, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(10, 50, strides=[50, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %onnx::Cast_5 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"Constant_0\"]() # /tmp/ipykernel_26179/2041700439.py:11:0\n",
      "  %onnx::Gather_13 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_1\"](%onnx::Cast_5) # /tmp/ipykernel_26179/2041700439.py:11:0\n",
      "  %onnx::Gather_6 : Float(3, 1, 82, strides=[82, 82, 1], requires_grad=0, device=cpu) = onnx::Gather[axis=0, onnx_name=\"Gather_2\"](%input, %onnx::Gather_13) # /tmp/ipykernel_26179/2041700439.py:11:0\n",
      "  %onnx::Cast_7 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"Constant_3\"]() # /tmp/ipykernel_26179/2041700439.py:13:0\n",
      "  %onnx::Gather_14 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7, onnx_name=\"Cast_4\"](%onnx::Cast_7) # /tmp/ipykernel_26179/2041700439.py:13:0\n",
      "  %onnx::Gemm_8 : Float(1, 82, strides=[82, 1], requires_grad=0, device=cpu) = onnx::Gather[axis=0, onnx_name=\"Gather_5\"](%onnx::Gather_6, %onnx::Gather_14) # /tmp/ipykernel_26179/2041700439.py:13:0\n",
      "  %onnx::Relu_9 : Float(1, 50, strides=[50, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_6\"](%onnx::Gemm_8, %fc1.weight, %fc1.bias) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %onnx::Gemm_10 : Float(1, 50, strides=[50, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_7\"](%onnx::Relu_9) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %x : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_8\"](%onnx::Gemm_10, %fc2.weight, %fc2.bias) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %output : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::LogSoftmax[axis=1, onnx_name=\"LogSoftmax_9\"](%x) # /home/sajjad/.local/lib/python3.9/site-packages/torch/nn/functional.py:1923:0\n",
      "  return (%output)\n",
      "\n",
      "------------- Checking exported model\n",
      "graph torch_jit (\n",
      "  %input[FLOAT, 1x3x1x82]\n",
      ") optional inputs with matching initializers (\n",
      "  %fc1.weight[FLOAT, 50x82]\n",
      "  %fc1.bias[FLOAT, 50]\n",
      "  %fc2.weight[FLOAT, 10x50]\n",
      "  %fc2.bias[FLOAT, 10]\n",
      ") {\n",
      "  %onnx::Cast_5 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Gather_13 = Cast[to = 7](%onnx::Cast_5)\n",
      "  %onnx::Gather_6 = Gather[axis = 0](%input, %onnx::Gather_13)\n",
      "  %onnx::Cast_7 = Constant[value = <Scalar Tensor []>]()\n",
      "  %onnx::Gather_14 = Cast[to = 7](%onnx::Cast_7)\n",
      "  %onnx::Gemm_8 = Gather[axis = 0](%onnx::Gather_6, %onnx::Gather_14)\n",
      "  %onnx::Relu_9 = Gemm[alpha = 1, beta = 1, transB = 1](%onnx::Gemm_8, %fc1.weight, %fc1.bias)\n",
      "  %onnx::Gemm_10 = Relu(%onnx::Relu_9)\n",
      "  %x = Gemm[alpha = 1, beta = 1, transB = 1](%onnx::Gemm_10, %fc2.weight, %fc2.bias)\n",
      "  %output = LogSoftmax[axis = 1](%x)\n",
      "  return %output\n",
      "}\n",
      "------------- Performing the profiling...\n",
      "vnn_mnist.c: In function ‘vnn_CreateMnist’:\n",
      "vnn_mnist.c:146:29: warning: unused variable ‘data’ [-Wunused-variable]\n",
      "  146 |     uint8_t *               data;\n",
      "      |                             ^~~~\n",
      "At top level:\n",
      "vnn_mnist.c:94:17: warning: ‘load_data’ defined but not used [-Wunused-function]\n",
      "   94 | static uint8_t* load_data\n",
      "      |                 ^~~~~~~~~\n",
      "------------- Performing the profiling done!\n",
      "------------- Parsing the profiling results...\n",
      "------------- Parsing the profiling results done!\n",
      "------------- auto_profile done!...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def doProfiling(model,\n",
    "                 loop=1,\n",
    "                 imgChannel=3,\n",
    "                 imgDimX=28,\n",
    "                 imgDimY=28,\n",
    "                 debug=False):\n",
    "    \"\"\"\n",
    "    Convert torch model to onnx model and get layer bits config of onnx model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch model\n",
    "        The model to speedup by quantization\n",
    "    loop : loop\n",
    "        the number of loops to run the model on khadas in main.c\n",
    "    imgChannel: input channels\n",
    "        input image channel\n",
    "    imgDimX: input width\n",
    "        image width\n",
    "    imgDimY: input height\n",
    "        image height\n",
    "    debug : show debugging\n",
    "        show the debugging output of the scripts\n",
    "    Returns\n",
    "    -------\n",
    "    pandas frame\n",
    "        contains the execution times (profiled time)\n",
    "    status\n",
    "        the error flag indicating the status\n",
    "    \"\"\"\n",
    "\n",
    "    perform_script_abs = script_path+\"/\"+perform_script\n",
    "    parse_script_abs = script_path+\"/\"+parse_script\n",
    "    perform_log_file_abs = log_path+\"/\"+perform_log_file\n",
    "    parsed_log_file_abs = log_path+\"/\"+parsed_log_file\n",
    "    model_with_Path = network_path+\"/\"+model_name+\".onnx\"\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    # choose the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ## generate 10 random input images based on the provided dimensions\n",
    "    generate_random_images(gen_img_input_dim_w, gen_img_input_dim_h, channels=3, count=20, path=quant_image_path)\n",
    "\n",
    "    [pArray,status] = auto_profile(model,\n",
    "                            loop,\n",
    "                            imgChannel,\n",
    "                            imgDimX,\n",
    "                            imgDimY,\n",
    "                            model_with_Path,\n",
    "                            test_input_data,\n",
    "                            perform_script_abs,\n",
    "                            parse_script_abs,\n",
    "                            perform_log_file_abs,\n",
    "                            parsed_log_file_abs,\n",
    "                            debug)\n",
    "\n",
    "    print(\"------------- auto_profile done!...\")\n",
    "\n",
    "    return pArray,status\n",
    "\n",
    "loop_run = '10'\n",
    "\n",
    "[ProfileArray,status] = doProfiling(model,\n",
    "                 loop_run,\n",
    "                 gen_img_input_channels,\n",
    "                 gen_img_input_dim_h,\n",
    "                 gen_img_input_dim_w,\n",
    "                 debug=True)\n",
    "\n",
    "accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec76fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_call(['../scripts/sajjad.sh'])\n",
    "#subprocess.check_call([\"../scripts/sajjad.sh\"])\n",
    "#!ls -la\n",
    "#!echo \"Hello\"\n",
    "#!bash ../scripts/perform_r6.sh 10  ../convertdemo/dataset/rand_3.jpg ../logs/model_execution.log > ../logs/jupyter.log\n",
    "#myscript=\"../scripts/sajjad.sh\"\n",
    "#!bash {myscript}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bb490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "['Create Neural Network' 'Verify Graph' 'Run the 1 time' 'Run the 2 time'\n",
      " 'Run the 3 time' 'Run the 4 time' 'Run the 5 time' 'Run the 6 time'\n",
      " 'Run the 7 time' 'Run the 8 time' 'Run the 9 time' 'Run the 10 time'\n",
      " 'Total   ' 'Average ']\n",
      "---------\n",
      "\n",
      "[' 128368us' ' 10031us' ' 29282.00us' ' 76.00us' ' 54.00us' ' 54.00us'\n",
      " ' 52.00us' ' 52.00us' ' 51.00us' ' 52.00us' ' 51.00us' ' 51.00us'\n",
      " ' 29820.00us' ' 2982.00us']\n",
      "---------\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "C0 = np.array(ProfileArray[0])\n",
    "C1 = np.array(ProfileArray[1])\n",
    "print('---------\\n')\n",
    "print(C0)\n",
    "print('---------\\n')\n",
    "print(C1)\n",
    "print('---------\\n')\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76785711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- auto_profile done!...\n",
      "Sajjad Hussain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mymodule as  modu\n",
    "x = np.array([2,4,6,8])\n",
    "modu.my_mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80f576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
