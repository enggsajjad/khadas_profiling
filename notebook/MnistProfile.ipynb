{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f945bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from nni.algorithms.compression.pytorch.quantization import LsqQuantizer, QAT_Quantizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.numpy_helper\n",
    "from math import ceil\n",
    "### markus\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbaa8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgSizeY=28\n",
    "imgSizeY= ceil(imgSizeY/4)*4\n",
    "\n",
    "imgSizeX=32\n",
    "imgSizeX= ceil(imgSizeX/4)*4\n",
    "\n",
    "\n",
    "gen_img_input_dim_x = imgSizeX\n",
    "gen_img_input_dim_y = imgSizeY\n",
    "gen_img_input_channels = 3\n",
    "#test input image\n",
    "test_input_data=\"../convertdemo/dataset/mnist2.jpg\"\n",
    "#Paths\n",
    "quant_image_path = \"../quantization_images\"\n",
    "script_path = \"../scripts\"\n",
    "log_path = \"../logs\"\n",
    "network_path = \"../convertdemo/network\"\n",
    "#Files\n",
    "perform_script = \"perform_r6.sh\"\n",
    "parse_script = \"parse_r1.sh\"\n",
    "perform_log_file = \"model_execution.log\"\n",
    "parsed_log_file = \"model_execution_parsed.log\"\n",
    "model_name=\"mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3c22",
   "metadata": {},
   "source": [
    "# Generate images based on some arbitrary input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4177221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = quant_image_path\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  # Create a new directory because it does not exist \n",
    "  os.makedirs(path)\n",
    "  print(\"The new directory quantization_images is created!\")\n",
    "\n",
    "#delete the pre generated bmp/jpg files\n",
    "filelist = [ f for f in os.listdir(quant_image_path) if (f.endswith(\".jpg\") or f.endswith(\".bmp\") ) ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(quant_image_path, f))\n",
    "\n",
    "\n",
    "def generate_random_images(xdim, ydim, channels=3, count=1, path=\".\"):\n",
    "    \"\"\"\n",
    "    This functions generates random bmp images to use for quantization given\n",
    "    a defined dimension\n",
    "        @xdim   .. width of images\n",
    "        @ydim   .. height of images\n",
    "        @count  .. number of images\n",
    "        @path   .. path of images\n",
    "    \"\"\"\n",
    "    for c in range(count):\n",
    "        rnd_img = np.random.randint(low=0,high=255, size=(xdim,ydim,channels),dtype=np.uint8) #imag.transpose((1,2,0)\n",
    "        imag_tp = np.ascontiguousarray(rnd_img, dtype=np.uint8)\n",
    "\n",
    "        pil_image = PIL.Image.frombytes('RGB',(xdim,ydim), imag_tp)\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".bmp\")\n",
    "        pil_image.save(path + \"/rand_\"+str(c)+\".jpg\")\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## generate 10 random input images based on the provided dimensions\n",
    "generate_random_images(gen_img_input_dim_x, gen_img_input_dim_y, channels=3, count=10, path=quant_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cb324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dffa107",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08ff6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Dataloader for MNIST Dataset\n",
    "\n",
    "resize=(imgSizeX, imgSizeY)\n",
    "## convert images from 1-color channel to 3-color channel images\n",
    "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,)), transforms.Lambda(lambda x: x.repeat(3, 1, 1) )])\n",
    "trans = transforms.Compose([transforms.Resize(resize),transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,)), transforms.Lambda(lambda x: x.repeat(3, 1, 1) )])\n",
    "\n",
    "root='data'\n",
    "# if not exist, download mnist dataset\n",
    "train_set = datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = datasets.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9e899",
   "metadata": {},
   "source": [
    "### Export and view image to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b461dd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 : (3, 32, 28) -> (32, 28, 3)\n",
      "0 : (3, 32, 28) -> (32, 28, 3)\n",
      "4 : (3, 32, 28) -> (32, 28, 3)\n",
      "1 : (3, 32, 28) -> (32, 28, 3)\n",
      "9 : (3, 32, 28) -> (32, 28, 3)\n",
      "2 : (3, 32, 28) -> (32, 28, 3)\n",
      "3 : (3, 32, 28) -> (32, 28, 3)\n",
      "6 : (3, 32, 28) -> (32, 28, 3)\n",
      "7 : (3, 32, 28) -> (32, 28, 3)\n",
      "8 : (3, 32, 28) -> (32, 28, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f66ee7130>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "numbers = set()\n",
    "for i in range(0,100):\n",
    "    image, label = train_set[i]\n",
    "\n",
    "    # export until all numbers are present\n",
    "    if len(numbers)==10:\n",
    "        break\n",
    "    if label not in numbers:\n",
    "        numbers.add(label)\n",
    "        # denormalize\n",
    "        imag=(np.array(image.tolist())+0.5) * 255\n",
    "        # shape image from CHW [RRRRR[..],GGG[...],BBB[...]] -> HWC (3x28x28  -> 28x28x3 [RGB,RGB,RGB,RGB,...])\n",
    "        imag_tp = np.ascontiguousarray( imag.transpose((1,2,0)), dtype=np.uint8)\n",
    "        print(f\"{label} : {imag.shape} -> {imag_tp.shape}\")\n",
    "       \n",
    "        #print(imag.shape)\n",
    "        #print(imag.astype(np.uint8))\n",
    "        pil_image = PIL.Image.frombytes('RGB',(imgSizeX,imgSizeY), imag_tp)\n",
    "        #pil_image.save(\"example_images_rgb\\\\\"+str(label)+\".bmp\")\n",
    "        pil_image.save(quant_image_path + \"/\"+model_name+str(label)+\".bmp\")\n",
    "        pil_image.save(quant_image_path + \"/\"+model_name+str(label)+\".jpg\")\n",
    "        \n",
    "# visualize the last image as example\n",
    "plt.imshow(imag_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d53a5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(image[:,:,0])\n",
    "#print( (imag_tp[:,:, 0].astype(float)-127) / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee7f05",
   "metadata": {},
   "source": [
    "# Build the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11f4b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://karanbirchahal.medium.com/how-to-quantise-an-mnist-network-to-8-bits-in-pytorch-no-retraining-required-from-scratch-39f634ac8459\n",
    "## we want true rgb data to be trained\n",
    "mnist = False\n",
    "if mnist:\n",
    "  num_channels = 1\n",
    "else:\n",
    "  num_channels = 3\n",
    "\n",
    "class Mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(num_channels, 20, 5, 1)\n",
    "        self.conv1 = nn.Conv2d(in_channels = gen_img_input_channels, out_channels = 20, kernel_size = 5, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5, 1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(20, 50, 1, 1)\n",
    "\n",
    "        f1 = int(imgSizeX/4) - 3\n",
    "        f2 = int(imgSizeY/4) - 3\n",
    "        self.fc1 = nn.Linear(f1*f2*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        f1 = int(imgSizeX/4) - 3\n",
    "        f2 = int(imgSizeY/4) - 3\n",
    "        x = x.view(-1, f1*f2*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9337bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "#test\n",
    "conv1test = nn.Conv2d(in_channels = gen_img_input_channels, out_channels = 20, kernel_size = 5, stride = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b7bce",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb3d2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,  device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('{:2.0f}%  Loss {}'.format(100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Loss: {}  Accuracy: {}%)\\n'.format(\n",
    "        test_loss, 100 * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d67f1",
   "metadata": {},
   "source": [
    "# Train the model for random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa6de45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mnist(\n",
       "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(20, 50, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_images(count=1, path=\".\", extension='*.bmp'):\n",
    "    \"\"\"\n",
    "    loads images as np arrays; no normalization\n",
    "    \"\"\"\n",
    "    imgs =  []\n",
    "    files = glob.glob(path + \"/\" + extension)\n",
    "    for i, f in enumerate(files):\n",
    "        img = np.asarray(PIL.Image.open(f))\n",
    "        imgs.append(img)\n",
    "\n",
    "        if i+1 > count:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "imgs = load_images(count=10,path=quant_image_path)\n",
    "\n",
    "plt.imshow(imgs[0])\n",
    "\n",
    "model.train()\n",
    "# for img in imgs[:1]:\n",
    "#     target = 1\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(imgs[0])\n",
    "#     loss = F.nll_loss(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a40f3c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch 0 #\n",
      " 0%  Loss 2.2985942363739014\n",
      " 1%  Loss 2.334524393081665\n",
      " 2%  Loss 2.331967353820801\n",
      " 2%  Loss 2.1803207397460938\n",
      " 3%  Loss 1.679300308227539\n",
      " 4%  Loss 0.44350990653038025\n",
      " 5%  Loss 1.0870174169540405\n",
      " 6%  Loss 0.44478973746299744\n",
      " 7%  Loss 0.8185885548591614\n",
      " 8%  Loss 0.12845294177532196\n",
      " 8%  Loss 0.3862129747867584\n",
      " 9%  Loss 0.1309726983308792\n",
      "10%  Loss 0.2894265949726105\n",
      "11%  Loss 0.11949855089187622\n",
      "12%  Loss 0.017776230350136757\n",
      "12%  Loss 0.5697534680366516\n",
      "13%  Loss 0.08740135282278061\n",
      "14%  Loss 0.01684807613492012\n",
      "15%  Loss 0.008860183879733086\n",
      "16%  Loss 0.02163112349808216\n",
      "17%  Loss 0.04798120632767677\n",
      "18%  Loss 0.10913238674402237\n",
      "18%  Loss 0.005032853689044714\n",
      "19%  Loss 0.049494095146656036\n",
      "20%  Loss 0.031055588275194168\n",
      "21%  Loss 0.0030853906646370888\n",
      "22%  Loss 0.001414109137840569\n",
      "22%  Loss 0.08494926989078522\n",
      "23%  Loss 0.004967852495610714\n",
      "24%  Loss 1.1758991479873657\n",
      "25%  Loss 0.0061455415561795235\n",
      "26%  Loss 0.5030335187911987\n",
      "27%  Loss 0.12902528047561646\n",
      "28%  Loss 0.022144431248307228\n",
      "28%  Loss 0.00018485318287275732\n",
      "29%  Loss 0.005259144119918346\n",
      "30%  Loss 0.3474888801574707\n",
      "31%  Loss 0.004482933320105076\n",
      "32%  Loss 0.0020201641600579023\n",
      "32%  Loss 0.0017593686934560537\n",
      "33%  Loss 0.0032577805686742067\n",
      "34%  Loss 0.009739910252392292\n",
      "35%  Loss 0.003722605062648654\n",
      "36%  Loss 0.012118926271796227\n",
      "37%  Loss 0.00016909543774090707\n",
      "38%  Loss 0.6199029684066772\n",
      "38%  Loss 0.10299015045166016\n",
      "39%  Loss 0.12209020555019379\n",
      "40%  Loss 0.0014438978396356106\n",
      "41%  Loss 0.004392624832689762\n",
      "42%  Loss 0.6553488373756409\n",
      "42%  Loss 0.00391159113496542\n",
      "43%  Loss 0.030300702899694443\n",
      "44%  Loss 0.003716530743986368\n",
      "45%  Loss 0.3332355320453644\n",
      "46%  Loss 0.03287732973694801\n",
      "47%  Loss 0.0006183714140206575\n",
      "48%  Loss 0.005223878659307957\n",
      "48%  Loss 0.40677958726882935\n",
      "49%  Loss 0.015448294579982758\n",
      "50%  Loss 0.887586236000061\n",
      "51%  Loss 2.173532009124756\n",
      "52%  Loss 0.05367407202720642\n",
      "52%  Loss 0.022756893187761307\n",
      "53%  Loss 0.007858699187636375\n",
      "54%  Loss 0.013698180206120014\n",
      "55%  Loss 0.02950599230825901\n",
      "56%  Loss 0.07838703691959381\n",
      "57%  Loss 0.0788009986281395\n",
      "58%  Loss 0.0017468907171860337\n",
      "58%  Loss 0.014908650889992714\n",
      "59%  Loss 0.15874186158180237\n",
      "60%  Loss 0.6185591816902161\n",
      "61%  Loss 0.2532302737236023\n",
      "62%  Loss 0.0005617860006168485\n",
      "62%  Loss 0.00016851513646543026\n",
      "63%  Loss 0.004671686328947544\n",
      "64%  Loss 0.004338625818490982\n",
      "65%  Loss 0.026852989569306374\n",
      "66%  Loss 0.5110073089599609\n",
      "67%  Loss 0.009423960000276566\n",
      "68%  Loss 0.0015526574570685625\n",
      "68%  Loss 0.03584329038858414\n",
      "69%  Loss 0.39339175820350647\n",
      "70%  Loss 0.003214352298527956\n",
      "71%  Loss 0.027551114559173584\n",
      "72%  Loss 0.005811032839119434\n",
      "72%  Loss 0.002747148275375366\n",
      "73%  Loss 0.00453388225287199\n",
      "74%  Loss 0.003403229173272848\n",
      "75%  Loss 0.18186548352241516\n",
      "76%  Loss 0.0009122836636379361\n",
      "77%  Loss 0.08361542224884033\n",
      "78%  Loss 0.0735108032822609\n",
      "78%  Loss 0.024941105395555496\n",
      "79%  Loss 0.023383207619190216\n",
      "80%  Loss 0.03680723160505295\n",
      "81%  Loss 0.0006971310940571129\n",
      "82%  Loss 0.1413552165031433\n",
      "82%  Loss 0.002938769990578294\n",
      "83%  Loss 0.12931793928146362\n",
      "84%  Loss 0.03131718188524246\n",
      "85%  Loss 0.0034219236113131046\n",
      "86%  Loss 0.004578712396323681\n",
      "87%  Loss 0.017939483746886253\n",
      "88%  Loss 0.03848136216402054\n",
      "88%  Loss 0.0038500328082591295\n",
      "89%  Loss 0.002241777954623103\n",
      "90%  Loss 0.003055490320548415\n",
      "91%  Loss 0.015508981421589851\n",
      "92%  Loss 0.0035360760521143675\n",
      "92%  Loss 0.0002165405312553048\n",
      "93%  Loss 0.04127290099859238\n",
      "94%  Loss 0.009706823155283928\n",
      "95%  Loss 0.09113180637359619\n",
      "96%  Loss 0.03753257542848587\n",
      "97%  Loss 0.0014358522603288293\n",
      "98%  Loss 0.0005064933211542666\n",
      "98%  Loss 0.00838644988834858\n",
      "99%  Loss 0.0013095755130052567\n",
      "Loss: 0.08895146536165824  Accuracy: 97.24%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# training only 30 epoches\n",
    "for epoch in range(1):#range(10):\n",
    "    print('# Epoch {} #'.format(epoch))\n",
    "    train(model,  device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f6abb",
   "metadata": {},
   "source": [
    "# Model Save and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2908e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main function of this page is to convert pytorch model to onnx model.\n",
    "Convertion from pytorch model to onnx model is primary so that a critical\n",
    "problem is caused that Layer name of pytorch model fail to convert to onnx\n",
    "layer name directly. To solve it, we wrap pytorch model in new wrapper which\n",
    "multiply bits number and input before computation of each op. Only in this\n",
    "way can onnx model get bits number of corresponded layer.\n",
    "\"\"\"\n",
    "\n",
    "class LayernameModuleWrapper(torch.nn.Module):\n",
    "    def __init__(self, module, module_bits) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        module : torch.nn.Module\n",
    "            Layer module of pytorch model\n",
    "        module_bits : int\n",
    "            Bits width setting for module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.module_bits = module_bits\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs*self.module_bits\n",
    "        inputs = self.module(inputs)\n",
    "        return inputs\n",
    "\n",
    "def _setattr(model, name, module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch model\n",
    "        The model to speedup by quantization\n",
    "    name : str\n",
    "        name of pytorch module\n",
    "    module : torch.nn.Module\n",
    "        Layer module of pytorch model\n",
    "    \"\"\"\n",
    "    name_list = name.split(\".\")\n",
    "    for name in name_list[:-1]:\n",
    "        model = getattr(model, name)\n",
    "    setattr(model, name_list[-1], module)\n",
    "\n",
    "def unwrapper(model_onnx, index2name, config):\n",
    "    \"\"\"\n",
    "    Fill onnx config and remove wrapper node in onnx\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_onnx : onnx model\n",
    "        Onnx model which is converted from pytorch model\n",
    "    index2name : dict\n",
    "        Dictionary of layer index and name\n",
    "    config : dict\n",
    "        Config recording name of layers and calibration parameters\n",
    "    Returns\n",
    "    -------\n",
    "    onnx model\n",
    "        Onnx model which is converted from pytorch model\n",
    "    dict\n",
    "        The configuration of onnx model layers and calibration parameters\n",
    "    \"\"\"\n",
    "    # Support Gemm, Conv, Relu, Clip(Relu6) and Maxpool\n",
    "    support_op = ['Gemm', 'Conv', 'Relu', 'Clip', 'MaxP']\n",
    "    idx = 0\n",
    "    onnx_config = {}\n",
    "    while idx < len(model_onnx.graph.node):\n",
    "        nd = model_onnx.graph.node[idx]\n",
    "        if nd.name[0:4] in support_op and  idx > 1:\n",
    "            # Grad constant node and multiply node\n",
    "            const_nd = model_onnx.graph.node[idx-2]\n",
    "            mul_nd = model_onnx.graph.node[idx-1]\n",
    "            # Get index number which is transferred by constant node\n",
    "            index = int(onnx.numpy_helper.to_array(const_nd.attribute[0].t))\n",
    "            if index != -1:\n",
    "                name = index2name[index]\n",
    "                onnx_config[nd.name] = config[name]\n",
    "            nd.input[0] = mul_nd.input[0]\n",
    "            # Remove constant node and multiply node\n",
    "            model_onnx.graph.node.remove(const_nd)\n",
    "            model_onnx.graph.node.remove(mul_nd)\n",
    "            idx = idx-2\n",
    "        idx = idx+1\n",
    "    return model_onnx, onnx_config\n",
    "\n",
    "def torch_to_onnx(model, input_shape, model_path, input_names, output_names):\n",
    "    \"\"\"\n",
    "    Convert torch model to onnx model and get layer bits config of onnx model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch model\n",
    "        The model to speedup by quantization\n",
    "    config : dict\n",
    "        Config recording bits number and name of layers\n",
    "    input_shape : tuple\n",
    "        The input shape of model, shall pass it to torch.onnx.export\n",
    "    model_path : str\n",
    "        The path user want to store onnx model which is converted from pytorch model\n",
    "    input_names : list\n",
    "        Input name of onnx model providing for torch.onnx.export to generate onnx model\n",
    "    output_name : list\n",
    "        Output name of onnx model providing for torch.onnx.export to generate onnx model\n",
    "    Returns\n",
    "    -------\n",
    "    onnx model\n",
    "        Onnx model which is converted from pytorch model\n",
    "    dict\n",
    "        The configuration of onnx model layers and calibration parameters\n",
    "    \"\"\"\n",
    "    # Convert torch model to onnx model and save it in model_path\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    model.to('cpu')\n",
    "    torch.onnx.export(model, dummy_input, model_path, verbose=False, input_names=input_names, output_names=output_names, export_params=True)\n",
    "\n",
    "    # Load onnx model\n",
    "    model_onnx = onnx.load(model_path)\n",
    "    model_onnx, onnx_config = unwrapper(model_onnx, index2name, config)\n",
    "    onnx.save(model_onnx, model_path)\n",
    "\n",
    "    onnx.checker.check_model(model_onnx)\n",
    "    return model_onnx, onnx_config\n",
    "\n",
    "\n",
    "\n",
    "def export_model_to_onnx(model, input_shape=(1,3,imgSizeX,imgSizeY), path=model_name+\".onnx\"):\n",
    "\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    model.to('cpu')\n",
    "        \n",
    "    # very important or must leave out - not sure need to test again...\n",
    "    #traced = torch.jit.trace(model, input_dimension)\n",
    "    print(\"------------- Exporting to onnx\")\n",
    "    torch.onnx.export(\n",
    "                      model, \n",
    "                      dummy_input, \n",
    "                      path,\n",
    "                      opset_version=7,\n",
    "                      verbose=True,\n",
    "                      export_params=True, \n",
    "                      input_names=['input'],\n",
    "                      output_names=['output'],\n",
    "                      dynamic_axes=None\n",
    "    )\n",
    "    \n",
    "    print(\"------------- Checking exported model\")\n",
    "    \n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(path)\n",
    "\n",
    "    # Check that the IR is well formed\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    # Print a Human readable representation of the graph\n",
    "    #print( onnx.helper.printable_graph(onnx_model.graph) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3cfa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_model_to_onnx(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d1c11",
   "metadata": {},
   "source": [
    "Create a image dimensions configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59e3379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(network_path+\"/\"+\"imgSize.config\", 'w') as f:\n",
    "    f.write('imgSize=%d,%d,%d'%(gen_img_input_channels,imgSizeX,imgSizeY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "788d7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import L\n",
    "import subprocess\n",
    "from subprocess import DEVNULL, STDOUT\n",
    "from xmlrpc.client import boolean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def parse_the_results(inp=\"model_execution.log\",\n",
    "                      out=\"model_execution_parsed.log\",\n",
    "                      script=\"parse.sh\",\n",
    "                      loop=1,\n",
    "                      show=False):\n",
    "    \"\"\"\n",
    "    parse the output of the profiled log\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp : input file\n",
    "        file to be parsed\n",
    "    out : output file\n",
    "        the output parsed file\n",
    "    scripts : parsing scripts\n",
    "        shell scripts to be used for parsing\n",
    "    loop: number of runs\n",
    "        the loops to run the model in the main.c\n",
    "    show: show the scripts output\n",
    "        to sohw or hide the shell script output\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if show == False:\n",
    "        subprocess.check_call([script, inp, out,loop], stdout=DEVNULL, stderr=STDOUT)\n",
    "    else:\n",
    "        subprocess.check_call([script, inp, out,loop])\n",
    "\n",
    "def run_profiler(script=script_path+\"/\"+\"perform.sh\",loop=1,test_input=\"test.jpg\",lgofile=\"model_execution_parsed.log\",show=False):\n",
    "    \"\"\"\n",
    "    profile the model\n",
    "    Parameters\n",
    "    ----------\n",
    "    scripts : parsing scripts\n",
    "        shell scripts to be used for parsing\n",
    "    loop: number of runs\n",
    "        the loops to run the model in the main.c\n",
    "    show: show the scripts output\n",
    "        to sohw or hide the shell script output\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if show == False:\n",
    "        subprocess.check_call([script, loop, test_input,lgofile], stdout=DEVNULL, stderr=STDOUT)\n",
    "    else:\n",
    "        subprocess.check_call([script, loop, test_input,lgofile])\n",
    "\n",
    "\n",
    "\n",
    "def auto_profile(model,\n",
    "                 loop=1,\n",
    "                 imgChannel=3,\n",
    "                 imgDimX=28,\n",
    "                 imgDimY=28,\n",
    "                 modelwithPath=model_name+\".onnx\",\n",
    "                 testingInput=\"../convertdemo/dataset/mnist2.jpg\",\n",
    "                 performScript = script_path+\"/\"+\"perform.sh\",\n",
    "                 parseScript = script_path+\"/\"+\"parse.sh\",\n",
    "                 performLogFile = log_path+\"/\"+\"model_execution.log\",\n",
    "                 parsedLogFile = log_path+\"/\"+\"model_execution_parsed.log\",\n",
    "                 debug=False):\n",
    "    \"\"\"\n",
    "    Convert torch model to onnx model and get layer bits config of onnx model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch model\n",
    "        The model to speedup by quantization\n",
    "    loop : loop\n",
    "        the number of loops to run the model on khadas in main.c\n",
    "    debug : show debugging\n",
    "        show the debugging output of the scripts\n",
    "    Returns\n",
    "    -------\n",
    "    pandas frame\n",
    "        contains the execution times (profiled time)\n",
    "    status\n",
    "        the error flag indicating the status\n",
    "    \"\"\"\n",
    "    profilingDone = False;\n",
    "    export_model_to_onnx(model,input_shape=(1,imgChannel,imgDimX,imgDimY), path=modelwithPath)\n",
    "\n",
    "    #sajjad@teco:~/sajjad/scripts/notebook$ ../scripts/perform_r6.sh 10 ../convertdemo/dataset/mnist2.jpg ../logs/model_execution.log\n",
    "    run_profiler(performScript,loop,testingInput,performLogFile,debug)\n",
    "\n",
    "    parse_the_results(performLogFile,parsedLogFile,parseScript,loop,debug)\n",
    "\n",
    "    #read the results into the pandas\n",
    "    profiledFrames=pd.read_csv(parsedLogFile, sep=':',header = None)\n",
    "\n",
    "    profilingDone = True;\n",
    "    return profiledFrames,profilingDone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "133ee91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Exporting to onnx\n",
      "graph(%input : Float(1, 3, 32, 28, strides=[2688, 896, 28, 1], requires_grad=0, device=cpu),\n",
      "      %conv1.weight : Float(20, 3, 5, 5, strides=[75, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv1.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2.weight : Float(20, 20, 5, 5, strides=[500, 25, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv2.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv3.weight : Float(50, 20, 1, 1, strides=[20, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %conv3.bias : Float(50, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc1.weight : Float(500, 1000, strides=[1000, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(500, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(10, 500, strides=[500, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %11 : Float(1, 20, 28, 24, strides=[13440, 672, 24, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%input, %conv1.weight, %conv1.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:442:0\n",
      "  %12 : Float(1, 20, 28, 24, strides=[13440, 672, 24, 1], requires_grad=1, device=cpu) = onnx::Relu(%11) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1299:0\n",
      "  %13 : Float(1, 20, 14, 12, strides=[3360, 168, 12, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%12) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:719:0\n",
      "  %14 : Float(1, 20, 10, 8, strides=[1600, 80, 8, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%13, %conv2.weight, %conv2.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:442:0\n",
      "  %15 : Float(1, 20, 10, 8, strides=[1600, 80, 8, 1], requires_grad=1, device=cpu) = onnx::Relu(%14) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1299:0\n",
      "  %16 : Float(1, 50, 10, 8, strides=[4000, 80, 8, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%15, %conv3.weight, %conv3.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:442:0\n",
      "  %17 : Float(1, 50, 10, 8, strides=[4000, 80, 8, 1], requires_grad=1, device=cpu) = onnx::Relu(%16) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1299:0\n",
      "  %18 : Float(1, 50, 5, 4, strides=[1000, 20, 4, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%17) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:719:0\n",
      "  %19 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=   -1  1000 [ CPUDoubleType{2} ]]()\n",
      "  %25 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Cast[to=7](%19)\n",
      "  %20 : Float(1, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Reshape(%18, %25) # /tmp/ipykernel_11174/3608531131.py:33:0\n",
      "  %21 : Float(1, 500, strides=[500, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%20, %fc1.weight, %fc1.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1848:0\n",
      "  %22 : Float(1, 500, strides=[500, 1], requires_grad=1, device=cpu) = onnx::Relu(%21) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1299:0\n",
      "  %23 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%22, %fc2.weight, %fc2.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1848:0\n",
      "  %output : Float(1, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::LogSoftmax[axis=1](%23) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1769:0\n",
      "  return (%output)\n",
      "\n",
      "------------- Checking exported model\n",
      "\u001b[35m>>>>> Setting Environment Variables Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Quantization Images and Model Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the Conversion Scripts Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Creating Remote NBG Directory... <<<<<<<\u001b[37m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the input device is not a TTY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m>>>>> Creating Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying C,H & NB files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying Template Makefile & Build files to Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "patching file main.c\n",
      "\u001b[35m>>>>> Patching main.c on Remote NBG Directory Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_pre_process.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_mnist.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vnn_mnist.c: In function ‘vnn_CreateMnist’:\n",
      "vnn_mnist.c:146:29: warning: unused variable ‘data’ [-Wunused-variable]\n",
      "  146 |     uint8_t *               data;\n",
      "      |                             ^~~~\n",
      "At top level:\n",
      "vnn_mnist.c:94:17: warning: ‘load_data’ defined but not used [-Wunused-function]\n",
      "   94 | static uint8_t* load_data\n",
      "      |                 ^~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COMPILE /home/khadas/nbg_unify_mnist/main.c\n",
      "  COMPILE /home/khadas/nbg_unify_mnist/vnn_post_process.c\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[35m>>>>> Compiling the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Copying test input image to Remote! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas... <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing the model on Remote Khadas Done! <<<<<<<\u001b[37m\n",
      "\u001b[35m>>>>> Executing network on Khadas is saved in model_execution.log <<<<<<<\u001b[37m\n",
      "Parsing ../logs/model_execution.log....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loop_run = '10'\n",
    "show_output = False\n",
    "show_output = True\n",
    "\n",
    "perform_script_abs = script_path+\"/\"+perform_script\n",
    "parse_script_abs = script_path+\"/\"+parse_script\n",
    "perform_log_file_abs = log_path+\"/\"+perform_log_file\n",
    "parsed_log_file_abs = log_path+\"/\"+parsed_log_file\n",
    "model_with_Path = network_path+\"/\"+model_name+\".onnx\"\n",
    "\n",
    "\n",
    "[ProfileArray,status] = auto_profile(model,\n",
    "                        loop_run,\n",
    "                        gen_img_input_channels,\n",
    "                        imgSizeX,\n",
    "                        imgSizeY,\n",
    "                        model_with_Path,\n",
    "                        test_input_data,\n",
    "                        perform_script_abs,\n",
    "                        parse_script_abs,\n",
    "                        perform_log_file_abs,\n",
    "                        parsed_log_file_abs,\n",
    "                        show_output)\n",
    "\n",
    "accData=ProfileArray.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62bb490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "\n",
      "['Create Neural Network' 'Verify Graph' 'Run the 1 time' 'Run the 2 time'\n",
      " 'Run the 3 time' 'Run the 4 time' 'Run the 5 time' 'Run the 6 time'\n",
      " 'Run the 7 time' 'Run the 8 time' 'Run the 9 time' 'Run the 10 time'\n",
      " 'Total   ' 'Average ']\n",
      "---------\n",
      "\n",
      "[' 110402us' ' 6251us' ' 18091.00us' ' 220.00us' ' 183.00us' ' 176.00us'\n",
      " ' 181.00us' ' 175.00us' ' 175.00us' ' 177.00us' ' 175.00us' ' 178.00us'\n",
      " ' 19862.00us' ' 1986.20us']\n",
      "---------\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "C0 = np.array(ProfileArray[0])\n",
    "C1 = np.array(ProfileArray[1])\n",
    "print('---------\\n')\n",
    "print(C0)\n",
    "print('---------\\n')\n",
    "print(C1)\n",
    "print('---------\\n')\n",
    "print(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
