/****************************************************************************
*   Generated by ACUITY 5.21.1_0702
*   Match ovxlib 1.1.30
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_mnist.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        if( NULL == _node ) {\
            goto error;\
        }\
        _node->uid = (uint32_t)_uid;\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR_FROM_HANDLE(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensorFromHandle( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (22)
#define NET_NORM_TENSOR_NUM     (2)
#define NET_CONST_TENSOR_NUM    (19)
#define NET_VIRTUAL_TENSOR_NUM  (22)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateMnist
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx,
    const vsi_nn_preprocess_map_element_t * pre_process_map,
    uint32_t pre_process_map_count,
    const vsi_nn_postprocess_map_element_t * post_process_map,
    uint32_t post_process_map_count
    )
{
    uint32_t                _infinity = VSI_NN_FLOAT32_INF;
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;
    uint32_t                i = 0;
    char *                  use_img_process_s;
    int32_t                 enable_pre_post_process = 0;
    vsi_bool                sort = FALSE;

    uint32_t   shape_1[] = { 320, 320, 128, 1 };




    (void)(_infinity);
    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;
    memset( &attr, 0, sizeof( attr ) );

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
    }
    else
    {
        ctx = in_ctx;
    }

    use_img_process_s = getenv( "VSI_USE_IMAGE_PROCESS" );
    if( use_img_process_s )
    {
        enable_pre_post_process = atoi(use_img_process_s);
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphVersion( graph, VNN_VERSION_MAJOR, VNN_VERSION_MINOR, VNN_VERSION_PATCH );
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 1 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - Constant_Cast_10_onnx__Gather_122_as_const_19
      var       - node[0]
      name      - Constant_Cast_10_onnx__Gather_122_as_const
      operation - variable
      input     - 
      output    - [1]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_VARIABLE, 1, 1, 19);

    /*-----------------------------------------
      lid       - Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14
      var       - node[1]
      name      - dtype_converter
      operation - dtype_converter
      input     - [1]
      output    - [1]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_DATACONVERT, 1, 1, 14);

    /*-----------------------------------------
      lid       - Gather_Gather_11_14
      var       - node[2]
      name      - Gather_Gather_11
      operation - gather
      input     - [12800, 1024, 3, 1]
                  [1]
      output    - [12800, 1024, 1, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_GATHER, 2, 1, 14);
    node[2]->nn_param.gather.axis = 2;

    /*-----------------------------------------
      lid       - Reshape_Reshape_23_10
      var       - node[3]
      name      - Reshape_Reshape_23
      operation - reshape
      input     - [12800, 1024, 1, 1]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_RESHAPE, 1, 1, 10);
    node[3]->nn_param.reshape.size = shape_1;
    node[3]->nn_param.reshape.dim_num = 4;

    /*-----------------------------------------
      lid       - Conv_Conv_24_20
      var       - node[4]
      name      - Conv_Conv_24
      operation - convolution
      input     - [320, 320, 128, 1]
      filter    - [1, 1, 128, 768]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV2D, 3, 1, 20);
    node[4]->nn_param.conv2d.ksize[0] = 1;
    node[4]->nn_param.conv2d.ksize[1] = 1;
    node[4]->nn_param.conv2d.weights = 768;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 0;
    node[4]->nn_param.conv2d.pad[1] = 0;
    node[4]->nn_param.conv2d.pad[2] = 0;
    node[4]->nn_param.conv2d.pad[3] = 0;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->nn_param.conv2d.multiplier = 0;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_25_18
      var       - node[5]
      name      - Relu_Relu_25
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_RELU, 1, 1, 18);

    /*-----------------------------------------
      lid       - Conv_Conv_26_17
      var       - node[6]
      name      - Conv_Conv_26
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [3, 3, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_CONV2D, 3, 1, 17);
    node[6]->nn_param.conv2d.ksize[0] = 3;
    node[6]->nn_param.conv2d.ksize[1] = 3;
    node[6]->nn_param.conv2d.weights = 768;
    node[6]->nn_param.conv2d.stride[0] = 1;
    node[6]->nn_param.conv2d.stride[1] = 1;
    node[6]->nn_param.conv2d.pad[0] = 1;
    node[6]->nn_param.conv2d.pad[1] = 1;
    node[6]->nn_param.conv2d.pad[2] = 1;
    node[6]->nn_param.conv2d.pad[3] = 1;
    node[6]->nn_param.conv2d.group = 768;
    node[6]->nn_param.conv2d.dilation[0] = 1;
    node[6]->nn_param.conv2d.dilation[1] = 1;
    node[6]->nn_param.conv2d.multiplier = 1;
    node[6]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[6]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_27_13
      var       - node[7]
      name      - Relu_Relu_27
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_RELU, 1, 1, 13);

    /*-----------------------------------------
      lid       - Conv_Conv_28_9
      var       - node[8]
      name      - Conv_Conv_28
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [1, 1, 768, 128]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_CONV2D, 3, 1, 9);
    node[8]->nn_param.conv2d.ksize[0] = 1;
    node[8]->nn_param.conv2d.ksize[1] = 1;
    node[8]->nn_param.conv2d.weights = 128;
    node[8]->nn_param.conv2d.stride[0] = 1;
    node[8]->nn_param.conv2d.stride[1] = 1;
    node[8]->nn_param.conv2d.pad[0] = 0;
    node[8]->nn_param.conv2d.pad[1] = 0;
    node[8]->nn_param.conv2d.pad[2] = 0;
    node[8]->nn_param.conv2d.pad[3] = 0;
    node[8]->nn_param.conv2d.group = 1;
    node[8]->nn_param.conv2d.dilation[0] = 1;
    node[8]->nn_param.conv2d.dilation[1] = 1;
    node[8]->nn_param.conv2d.multiplier = 0;
    node[8]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[8]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[8]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Add_Add_29_6
      var       - node[9]
      name      - Add_Add_29
      operation - add
      input     - [320, 320, 128, 1]
                  [320, 320, 128, 1]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_ADD, 2, 1, 6);

    /*-----------------------------------------
      lid       - Conv_Conv_30_21
      var       - node[10]
      name      - Conv_Conv_30
      operation - convolution
      input     - [320, 320, 128, 1]
      filter    - [1, 1, 128, 768]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_CONV2D, 3, 1, 21);
    node[10]->nn_param.conv2d.ksize[0] = 1;
    node[10]->nn_param.conv2d.ksize[1] = 1;
    node[10]->nn_param.conv2d.weights = 768;
    node[10]->nn_param.conv2d.stride[0] = 1;
    node[10]->nn_param.conv2d.stride[1] = 1;
    node[10]->nn_param.conv2d.pad[0] = 0;
    node[10]->nn_param.conv2d.pad[1] = 0;
    node[10]->nn_param.conv2d.pad[2] = 0;
    node[10]->nn_param.conv2d.pad[3] = 0;
    node[10]->nn_param.conv2d.group = 1;
    node[10]->nn_param.conv2d.dilation[0] = 1;
    node[10]->nn_param.conv2d.dilation[1] = 1;
    node[10]->nn_param.conv2d.multiplier = 0;
    node[10]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[10]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[10]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_31_16
      var       - node[11]
      name      - Relu_Relu_31
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_RELU, 1, 1, 16);

    /*-----------------------------------------
      lid       - Conv_Conv_32_12
      var       - node[12]
      name      - Conv_Conv_32
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [3, 3, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_CONV2D, 3, 1, 12);
    node[12]->nn_param.conv2d.ksize[0] = 3;
    node[12]->nn_param.conv2d.ksize[1] = 3;
    node[12]->nn_param.conv2d.weights = 768;
    node[12]->nn_param.conv2d.stride[0] = 1;
    node[12]->nn_param.conv2d.stride[1] = 1;
    node[12]->nn_param.conv2d.pad[0] = 1;
    node[12]->nn_param.conv2d.pad[1] = 1;
    node[12]->nn_param.conv2d.pad[2] = 1;
    node[12]->nn_param.conv2d.pad[3] = 1;
    node[12]->nn_param.conv2d.group = 768;
    node[12]->nn_param.conv2d.dilation[0] = 1;
    node[12]->nn_param.conv2d.dilation[1] = 1;
    node[12]->nn_param.conv2d.multiplier = 1;
    node[12]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[12]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[12]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_33_8
      var       - node[13]
      name      - Relu_Relu_33
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_RELU, 1, 1, 8);

    /*-----------------------------------------
      lid       - Conv_Conv_34_5
      var       - node[14]
      name      - Conv_Conv_34
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [1, 1, 768, 128]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_CONV2D, 3, 1, 5);
    node[14]->nn_param.conv2d.ksize[0] = 1;
    node[14]->nn_param.conv2d.ksize[1] = 1;
    node[14]->nn_param.conv2d.weights = 128;
    node[14]->nn_param.conv2d.stride[0] = 1;
    node[14]->nn_param.conv2d.stride[1] = 1;
    node[14]->nn_param.conv2d.pad[0] = 0;
    node[14]->nn_param.conv2d.pad[1] = 0;
    node[14]->nn_param.conv2d.pad[2] = 0;
    node[14]->nn_param.conv2d.pad[3] = 0;
    node[14]->nn_param.conv2d.group = 1;
    node[14]->nn_param.conv2d.dilation[0] = 1;
    node[14]->nn_param.conv2d.dilation[1] = 1;
    node[14]->nn_param.conv2d.multiplier = 0;
    node[14]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[14]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[14]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Add_Add_35_3
      var       - node[15]
      name      - Add_Add_35
      operation - add
      input     - [320, 320, 128, 1]
                  [320, 320, 128, 1]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_ADD, 2, 1, 3);

    /*-----------------------------------------
      lid       - Conv_Conv_36_15
      var       - node[16]
      name      - Conv_Conv_36
      operation - convolution
      input     - [320, 320, 128, 1]
      filter    - [1, 1, 128, 768]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_CONV2D, 3, 1, 15);
    node[16]->nn_param.conv2d.ksize[0] = 1;
    node[16]->nn_param.conv2d.ksize[1] = 1;
    node[16]->nn_param.conv2d.weights = 768;
    node[16]->nn_param.conv2d.stride[0] = 1;
    node[16]->nn_param.conv2d.stride[1] = 1;
    node[16]->nn_param.conv2d.pad[0] = 0;
    node[16]->nn_param.conv2d.pad[1] = 0;
    node[16]->nn_param.conv2d.pad[2] = 0;
    node[16]->nn_param.conv2d.pad[3] = 0;
    node[16]->nn_param.conv2d.group = 1;
    node[16]->nn_param.conv2d.dilation[0] = 1;
    node[16]->nn_param.conv2d.dilation[1] = 1;
    node[16]->nn_param.conv2d.multiplier = 0;
    node[16]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[16]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[16]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_37_11
      var       - node[17]
      name      - Relu_Relu_37
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_RELU, 1, 1, 11);

    /*-----------------------------------------
      lid       - Conv_Conv_38_7
      var       - node[18]
      name      - Conv_Conv_38
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [3, 3, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_CONV2D, 3, 1, 7);
    node[18]->nn_param.conv2d.ksize[0] = 3;
    node[18]->nn_param.conv2d.ksize[1] = 3;
    node[18]->nn_param.conv2d.weights = 768;
    node[18]->nn_param.conv2d.stride[0] = 1;
    node[18]->nn_param.conv2d.stride[1] = 1;
    node[18]->nn_param.conv2d.pad[0] = 1;
    node[18]->nn_param.conv2d.pad[1] = 1;
    node[18]->nn_param.conv2d.pad[2] = 1;
    node[18]->nn_param.conv2d.pad[3] = 1;
    node[18]->nn_param.conv2d.group = 768;
    node[18]->nn_param.conv2d.dilation[0] = 1;
    node[18]->nn_param.conv2d.dilation[1] = 1;
    node[18]->nn_param.conv2d.multiplier = 1;
    node[18]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[18]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[18]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Relu_Relu_39_4
      var       - node[19]
      name      - Relu_Relu_39
      operation - relu
      input     - [320, 320, 768, 1]
      output    - [320, 320, 768, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_RELU, 1, 1, 4);

    /*-----------------------------------------
      lid       - Conv_Conv_40_2
      var       - node[20]
      name      - Conv_Conv_40
      operation - convolution
      input     - [320, 320, 768, 1]
      filter    - [1, 1, 768, 128]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_CONV2D, 3, 1, 2);
    node[20]->nn_param.conv2d.ksize[0] = 1;
    node[20]->nn_param.conv2d.ksize[1] = 1;
    node[20]->nn_param.conv2d.weights = 128;
    node[20]->nn_param.conv2d.stride[0] = 1;
    node[20]->nn_param.conv2d.stride[1] = 1;
    node[20]->nn_param.conv2d.pad[0] = 0;
    node[20]->nn_param.conv2d.pad[1] = 0;
    node[20]->nn_param.conv2d.pad[2] = 0;
    node[20]->nn_param.conv2d.pad[3] = 0;
    node[20]->nn_param.conv2d.group = 1;
    node[20]->nn_param.conv2d.dilation[0] = 1;
    node[20]->nn_param.conv2d.dilation[1] = 1;
    node[20]->nn_param.conv2d.multiplier = 0;
    node[20]->vx_param.overflow_policy = VX_CONVERT_POLICY_SATURATE;
    node[20]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[20]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - Add_Add_41_1
      var       - node[21]
      name      - Add_Add_41
      operation - add
      input     - [320, 320, 128, 1]
                  [320, 320, 128, 1]
      output    - [320, 320, 128, 1]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_ADD, 2, 1, 1);


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @attach_Add_Add_41/out0_0:out0 */
    attr.size[0] = 320;
    attr.size[1] = 320;
    attr.size[2] = 128;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.scale = 0.004727776627987623;
    attr.dtype.zero_point = 126;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_UINT8);

    /* @input_22:out0 */
    attr.size[0] = 12800;
    attr.size[1] = 1024;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.scale = 0.003906190162524581;
    attr.dtype.zero_point = 128;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_UINT8);



    /* @Constant_Cast_10_onnx__Gather_122_as_const_19:data */
    attr.size[0] = 1;
    attr.dim_num = 1;
    attr.dtype.scale = 1.0;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_UINT8, 0, 1);

    /* @Conv_Conv_24_20:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 128;
    attr.size[3] = 768;
    attr.dim_num = 4;
    attr.dtype.scale = 0.0016790616791695356;
    attr.dtype.zero_point = 126;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_UINT8, 3073, 98304);

    /* @Conv_Conv_24_20:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 6.558734213444044e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_26_17:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 768;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.scale = 0.0005429667653515935;
    attr.dtype.zero_point = 142;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_UINT8, 101377, 6912);

    /* @Conv_Conv_26_17:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 1.9848983273166275e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_28_9:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 768;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.scale = 0.004433945287019014;
    attr.dtype.zero_point = 141;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_UINT8, 108801, 98304);

    /* @Conv_Conv_28_9:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.scale = 9.400613705111589e-07;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_INT32, 108289, 512);

    /* @Conv_Conv_30_21:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 128;
    attr.size[3] = 768;
    attr.dim_num = 4;
    attr.dtype.scale = 0.001745095825754106;
    attr.dtype.zero_point = 130;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_UINT8, 207105, 98304);

    /* @Conv_Conv_30_21:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 7.75256211562529e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_32_12:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 768;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.scale = 0.0005272075068205595;
    attr.dtype.zero_point = 133;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_UINT8, 305409, 6912);

    /* @Conv_Conv_32_12:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 1.981923010655395e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_34_5:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 768;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.scale = 0.00431971438229084;
    attr.dtype.zero_point = 124;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_UINT8, 312321, 98304);

    /* @Conv_Conv_34_5:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.scale = 1.0922712309380548e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_INT32, 108289, 512);

    /* @Conv_Conv_36_15:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 128;
    attr.size[3] = 768;
    attr.dim_num = 4;
    attr.dtype.scale = 0.0017753362189978361;
    attr.dtype.zero_point = 119;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_UINT8, 410625, 98304);

    /* @Conv_Conv_36_15:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 8.14422491747166e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_38_7:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 768;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.scale = 0.0004951379960402846;
    attr.dtype.zero_point = 136;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_UINT8, 508929, 6912);

    /* @Conv_Conv_38_7:bias */
    attr.size[0] = 768;
    attr.dim_num = 1;
    attr.dtype.scale = 1.921202968143185e-06;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_INT32, 1, 3072);

    /* @Conv_Conv_40_2:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 768;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.scale = 0.004323011264204979;
    attr.dtype.zero_point = 116;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_UINT8, 515841, 98304);

    /* @Conv_Conv_40_2:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.scale = 8.837504481153775e-07;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_INT32, 108289, 512);



    /* @Constant_Cast_10_onnx__Gather_122_as_const_19:out0 */
    attr.dtype.scale = 1.0;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14:out0 */
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_NONE;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT32);

    /* @Gather_Gather_11_14:out0 */
    attr.dtype.scale = 0.003906190162524581;
    attr.dtype.zero_point = 128;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Reshape_Reshape_23_10:out0 */
    attr.dtype.scale = 0.003906190162524581;
    attr.dtype.zero_point = 128;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_24_20:out0 */
    attr.dtype.scale = 0.0036556534469127655;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_25_18:out0 */
    attr.dtype.scale = 0.0036556534469127655;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_26_17:out0 */
    attr.dtype.scale = 0.00021201465278863907;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_27_13:out0 */
    attr.dtype.scale = 0.00021201465278863907;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_28_9:out0 */
    attr.dtype.scale = 0.0006292708567343652;
    attr.dtype.zero_point = 129;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Add_Add_29_6:out0 */
    attr.dtype.scale = 0.004442485049366951;
    attr.dtype.zero_point = 128;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_30_21:out0 */
    attr.dtype.scale = 0.0037592845037579536;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_31_16:out0 */
    attr.dtype.scale = 0.0037592845037579536;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_32_12:out0 */
    attr.dtype.scale = 0.00025285728042945266;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_33_8:out0 */
    attr.dtype.scale = 0.00025285728042945266;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_34_5:out0 */
    attr.dtype.scale = 0.0006311304168775678;
    attr.dtype.zero_point = 129;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Add_Add_35_3:out0 */
    attr.dtype.scale = 0.004587426781654358;
    attr.dtype.zero_point = 128;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_36_15:out0 */
    attr.dtype.scale = 0.003880136413499713;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_37_11:out0 */
    attr.dtype.scale = 0.003880136413499713;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_38_7:out0 */
    attr.dtype.scale = 0.00020442936511244625;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Relu_Relu_39_4:out0 */
    attr.dtype.scale = 0.00020442936511244625;
    attr.dtype.zero_point = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);

    /* @Conv_Conv_40_2:out0 */
    attr.dtype.scale = 0.0006249690777622163;
    attr.dtype.zero_point = 123;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_AFFINE_ASYMMETRIC;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_UINT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[2]->input.tensors[0] = norm_tensor[1];
    node[21]->output.tensors[0] = norm_tensor[0];

    /* Constant_Cast_10_onnx__Gather_122_as_const_19 */
    node[0]->input.tensors[0] = const_tensor[0]; /* data_data */

    /* Constant_Cast_10_onnx__Gather_122_as_const_19_dtype_convert_Gather_Gather_11_14 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];

    /* Gather_Gather_11_14 */
    node[2]->input.tensors[1] = node[1]->output.tensors[0];

    /* Reshape_Reshape_23_10 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];

    /* Conv_Conv_24_20 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[1]; /* data_weight */
    node[4]->input.tensors[2] = const_tensor[2]; /* data_bias */

    /* Relu_Relu_25_18 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];

    /* Conv_Conv_26_17 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];
    node[6]->input.tensors[1] = const_tensor[3]; /* data_weight */
    node[6]->input.tensors[2] = const_tensor[4]; /* data_bias */

    /* Relu_Relu_27_13 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];

    /* Conv_Conv_28_9 */
    node[8]->input.tensors[0] = node[7]->output.tensors[0];
    node[8]->input.tensors[1] = const_tensor[5]; /* data_weight */
    node[8]->input.tensors[2] = const_tensor[6]; /* data_bias */

    /* Add_Add_29_6 */
    node[9]->input.tensors[0] = node[8]->output.tensors[0];
    node[9]->input.tensors[1] = node[3]->output.tensors[0];

    /* Conv_Conv_30_21 */
    node[10]->input.tensors[0] = node[9]->output.tensors[0];
    node[10]->input.tensors[1] = const_tensor[7]; /* data_weight */
    node[10]->input.tensors[2] = const_tensor[8]; /* data_bias */

    /* Relu_Relu_31_16 */
    node[11]->input.tensors[0] = node[10]->output.tensors[0];

    /* Conv_Conv_32_12 */
    node[12]->input.tensors[0] = node[11]->output.tensors[0];
    node[12]->input.tensors[1] = const_tensor[9]; /* data_weight */
    node[12]->input.tensors[2] = const_tensor[10]; /* data_bias */

    /* Relu_Relu_33_8 */
    node[13]->input.tensors[0] = node[12]->output.tensors[0];

    /* Conv_Conv_34_5 */
    node[14]->input.tensors[0] = node[13]->output.tensors[0];
    node[14]->input.tensors[1] = const_tensor[11]; /* data_weight */
    node[14]->input.tensors[2] = const_tensor[12]; /* data_bias */

    /* Add_Add_35_3 */
    node[15]->input.tensors[0] = node[14]->output.tensors[0];
    node[15]->input.tensors[1] = node[9]->output.tensors[0];

    /* Conv_Conv_36_15 */
    node[16]->input.tensors[0] = node[15]->output.tensors[0];
    node[16]->input.tensors[1] = const_tensor[13]; /* data_weight */
    node[16]->input.tensors[2] = const_tensor[14]; /* data_bias */

    /* Relu_Relu_37_11 */
    node[17]->input.tensors[0] = node[16]->output.tensors[0];

    /* Conv_Conv_38_7 */
    node[18]->input.tensors[0] = node[17]->output.tensors[0];
    node[18]->input.tensors[1] = const_tensor[15]; /* data_weight */
    node[18]->input.tensors[2] = const_tensor[16]; /* data_bias */

    /* Relu_Relu_39_4 */
    node[19]->input.tensors[0] = node[18]->output.tensors[0];

    /* Conv_Conv_40_2 */
    node[20]->input.tensors[0] = node[19]->output.tensors[0];
    node[20]->input.tensors[1] = const_tensor[17]; /* data_weight */
    node[20]->input.tensors[2] = const_tensor[18]; /* data_bias */

    /* Add_Add_41_1 */
    node[21]->input.tensors[0] = node[20]->output.tensors[0];
    node[21]->input.tensors[1] = node[15]->output.tensors[0];


    graph->output.tensors[0] = norm_tensor[0];
    graph->input.tensors[0] = norm_tensor[1];


    if( enable_pre_post_process )
    {
        sort = TRUE;
        if( pre_process_map_count > 0 )
        {
            for( i = 0; i < pre_process_map_count; i++ )
            {
                status = vsi_nn_AddGraphPreProcess(graph, pre_process_map[i].graph_input_idx,
                                                   pre_process_map[i].preprocesses,
                                                   pre_process_map[i].preprocess_count);
                TEST_CHECK_STATUS( status, error );
            }
        }

        if( post_process_map_count > 0 )
        {
            for( i = 0; i < post_process_map_count; i++ )
            {
                 status = vsi_nn_AddGraphPostProcess(graph, post_process_map[i].graph_output_idx,
                                                     post_process_map[i].postprocesses,
                                                     post_process_map[i].postprocess_count);
                 TEST_CHECK_STATUS( status, error );
            }
        }
    }

    status = vsi_nn_SetupGraph( graph, sort );
    TEST_CHECK_STATUS( status, error );
    vsi_nn_DumpGraphToJson( graph );

    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vsi_nn_DumpGraphToJson( graph );
    vnn_ReleaseMnist( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateMnist() */

void vnn_ReleaseMnist
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseMnist() */

